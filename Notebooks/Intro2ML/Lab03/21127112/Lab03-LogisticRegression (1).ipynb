{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab03: Logistic Regression.\n",
    "\n",
    "- Student ID: Huỳnh Sỉ Kha\n",
    "- Student name: 21127734"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`). Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "- Logistic Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('mnist_784', return_X_y=True, parser='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784)\n",
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we basically have 70000 samples with each sample having 784 features - pixels in this case and a label - the digit the image represent.\n",
    "\n",
    "Let’s play around and see if we can extract any features from the pixels that can be more informative. First I’d like to know more about average intensity - that is the average value of a pixel in an image for the different digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44.17740512 19.40680177 38.03420776 36.15420938 30.99599983 32.95015873\n",
      " 35.23486491 29.21798737 38.39790125 31.35940809]\n"
     ]
    }
   ],
   "source": [
    "labels=np.unique(y)\n",
    "# print(labels)\n",
    "n_label=np.unique(y).shape[0]\n",
    "l_means=np.zeros(shape=n_label,dtype=float) # array stores average intensity for each label\n",
    "\n",
    "#TODO compute average intensity for each label\n",
    "for i in range(n_label):\n",
    "    l_means[i]=np.mean(X[y==labels[i]])\n",
    "\n",
    "print(l_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the average intensity using matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 10 artists>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "ax.bar(labels,l_means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see there are some differences in intensity. The digit “1” is the less intense while the digit “0” is the most intense. So this new feature seems to have some predictive value if you wanted to know if say your digit is a “1” or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "#TODO compute average intensity for each data sample\n",
    "# intensity=?\n",
    "intensity=np.mean(X,axis=1)\n",
    "\n",
    "print(intensity.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes people really do not know what are they doing. I am not an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000,)\n"
     ]
    }
   ],
   "source": [
    "X = X.values\n",
    "X_flip=np.flip(X, axis=1)\n",
    "symmetry= np.mean(np.abs(X-X_flip),axis=1)\n",
    "print(symmetry.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I called this feature \"symmetry\" (though it's not \"symmetry\" at all). Use visualization method to understand why this thing works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new trainning data will have 70000 samples and 2 features: intensity, symmetry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 2)\n"
     ]
    }
   ],
   "source": [
    "# TODO create X_new by horizontal stack intensity and symmetry\n",
    "X_new = np.column_stack((intensity, symmetry))\n",
    "\n",
    "print(X_new.shape)  # it should be (70000,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually logistic regression is a good first choice for classification. In this homework we use logistic regression for classifying digit 1 images and not digit 1 images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize data\n",
    "First normalize data using Z-score normalization\n",
    "- **TODO: Study about Z-score normalization**\n",
    "\n",
    "#### Concept:\n",
    "\n",
    "- **Formula**: The Z-score of a value is calculated using the formula: \n",
    "  $$ Z = \\frac{(X - \\mu)}{\\sigma} $$\n",
    "  - $ X $ is the value to be normalized \n",
    "  - $ \\mu $ is the mean of the dataset\n",
    "  - $ \\sigma $ is the standard deviation of the dataset.\n",
    "\n",
    "#### Significance:\n",
    "- Mean and standard deviation: After normalization, the data will have a mean (average) of 0 and a standard deviation of 1.\n",
    "- Comparing and combining data: This technique helps to bring features with different ranges of values to a common scale, making it more objective and effective to compare and combine them in machine learning models.\n",
    "\n",
    "#### Applications:\n",
    "\n",
    "- In machine learning: Z-score normalization is commonly used in machine learning algorithms, especially those sensitive to the scale of data, such as K-means clustering or deep learning models.\n",
    "- Data preprocessing before model training: Normalizing data can speed up the convergence of machine learning algorithms and improve model performance.\n",
    "\n",
    "- **TODO: Why should we normalize data?**\n",
    "\n",
    "1. Equalizes feature scales: Normalization ensures that each feature contributes equally to the analysis by bringing different scales to a uniform level.\n",
    "\n",
    "2. Boosts gradient descent efficiency: In algorithms using gradient descent, normalization speeds up convergence by transforming the optimization landscape into a more symmetric shape.\n",
    "\n",
    "3. Mitigates outlier influence: By scaling data to a smaller range, the excessive impact of outliers is reduced.\n",
    "\n",
    "4. Improves algorithm performance: Algorithms like K-Nearest Neighbors and K-Means, which rely on distance calculations, perform better when all features are normalized.\n",
    "\n",
    "5. Stabilizes neural network training: Normalization helps meet neural network input expectations, aiding in better training performance and stability.\n",
    "\n",
    "6. Prevents numerical instabilities: Normalizing can help avoid issues related to how computers handle floating-point numbers, especially in high-dimensional data.\n",
    "\n",
    "7. Aligns with model assumptions: Certain algorithms assume normally distributed features; normalization helps approximate these conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: normalize X_new\n",
    "\n",
    "X_new = (X_new - np.mean(X_new, axis=0)) / np.std(X_new, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.hstack((np.ones((len(X_new), 1)), X_new)) #stack 1s column as usual\n",
    "y_new=y.astype(int)\n",
    "y_new[y_new != 1] = 0 # digit 1 -> class 1, other digits -> class 0\n",
    "# Convert y_new to a 2D array with one column (Numpy)\n",
    "y_new = y_new.to_numpy()\n",
    "y_new=y_new.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46667, 3)\n",
      "(46667, 1)\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(X_new, y_new, test_size= int(1/3*X.shape[0]))\n",
    "print(train_X.shape)\n",
    "print(train_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid function and derivative of the sigmoid function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_activation(x):\n",
    "    \"\"\"compute the sigmoid activation value for a given input\"\"\"\n",
    "    # Clip the input values to a specific range to prevent overflow\n",
    "    clipped_x = np.clip(x, -500, 500)\n",
    "    \n",
    "    # Compute the sigmoid using the clipped input (clipped_x)\n",
    "    return 1.0 / (1 + np.exp(-clipped_x))\n",
    "\n",
    "def sigmoid_deriv(x):\n",
    "    '''compute the derivative of the sigmoid function ASSUMING\n",
    "    that the input ‘x‘ has already been passed through the sigmoid\n",
    "    activation function'''\n",
    "    return x * (1 - x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_h(W, X):\n",
    "    \"\"\"\n",
    "    Compute output: Take the dot product between our features ‘X‘ and the weight\n",
    "    matrix ‘W‘, then pass this value through our sigmoid activation function \n",
    "    \"\"\"\n",
    "    return sigmoid_activation(X.dot(W))\n",
    "def predict(W, X):\n",
    " \n",
    "    '''Take the dot product between our features and weight matrix, \n",
    "       then pass this value through our sigmoid activation'''\n",
    "    #........\n",
    "    preds=sigmoid_activation(X.dot(W))\n",
    "    # apply a step function to threshold the outputs to binary\n",
    "    # class labels\n",
    "    preds[preds <= 0.5] = 0\n",
    "    preds[preds > 0.5] = 1\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function: Average negative log likelihood**\n",
    "$$\\mathcal{L}=\\dfrac{1}{N} \\sum_{i=1}^{N} -\\left(y^{i}\\ln h_{\\mathbf{w}}\\left(\\mathbf{x}^{i}\\right)+\\left(1-y^{i}\\right)\\ln \\left(1-h_{\\mathbf{w}}\\left(x^{i}\\right)\\right)\\right) $$\n",
    "\n",
    "\n",
    "$$\\text{Sigmoid Activation: } z= \\sigma \\left(h\\right)= \\dfrac{1}{1+e^{-h}}$$\n",
    "\n",
    "$$\\text{Cross-entropy: } J(w)=-\\left({ylog(z)+(1-y)log(1-z)}\\right)$$\n",
    "\n",
    "$$\\text{Chain rule: } \\dfrac{\\partial J(w)}{\\partial w}=\\dfrac{\\partial J(w)}{\\partial z} \\dfrac{\\partial z}{\\partial h}\\dfrac{\\partial h}{\\partial w}  $$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial z}=-\\left(\\dfrac{y}{z}-\\dfrac{1-y}{1-z}\\right)=\\dfrac{z-y}{z(1-z)}$$\n",
    "\n",
    "$$\\dfrac{\\partial z}{\\partial h}=z(1-z)$$\n",
    "\n",
    "$$\\dfrac{\\partial h}{\\partial w}=X$$\n",
    "\n",
    "$$\\dfrac{\\partial J(w)}{\\partial w}=X^T(z-y)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, error):\n",
    "    \"\"\"\n",
    "    This is the gradient descent update of \"average negative loglikelihood\" loss function. \n",
    "    In lab02 our loss function is \"sum squared error\".\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    gradient = X.T.dot(error)\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevent divide by zero\n",
    "def safe_log(x):\n",
    "    return np.log(np.clip(x, 1e-10, 1 - 1e-10))\n",
    "\n",
    "def train(W,train_X, train_y, learning_rate, num_epochs, losses):\n",
    "    for epoch in np.arange(0, num_epochs):\n",
    "        h=compute_h(W,train_X)\n",
    "        error = h - train_y\n",
    "        # loss = np.mean(- train_y * np.log(h) - (1 - train_y) * np.log(1 - h))\n",
    "        loss = np.mean(- train_y * safe_log(h) - (1 - train_y) * safe_log(1 - h))\n",
    "        losses.append(loss)\n",
    "        gradient=compute_gradient(train_X, error)\n",
    "        W += -learning_rate * gradient\n",
    "        if ((epoch+1)%1000==0): print ('Epoch %d, loss %.3f' %(epoch+1, loss))\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000, loss 1.312\n",
      "Epoch 2000, loss 1.312\n",
      "Epoch 3000, loss 1.312\n",
      "Epoch 4000, loss 1.312\n",
      "Epoch 5000, loss 1.312\n",
      "Epoch 6000, loss 1.312\n",
      "Epoch 7000, loss 1.312\n",
      "Epoch 8000, loss 1.312\n",
      "Epoch 9000, loss 1.312\n",
      "Epoch 10000, loss 1.312\n",
      "Epoch 11000, loss 1.312\n",
      "Epoch 12000, loss 1.312\n",
      "Epoch 13000, loss 1.312\n",
      "Epoch 14000, loss 1.312\n",
      "Epoch 15000, loss 1.312\n",
      "Epoch 16000, loss 1.312\n",
      "Epoch 17000, loss 1.312\n",
      "Epoch 18000, loss 1.312\n",
      "Epoch 19000, loss 1.312\n",
      "Epoch 20000, loss 1.312\n",
      "Epoch 21000, loss 1.312\n",
      "Epoch 22000, loss 1.312\n",
      "Epoch 23000, loss 1.312\n",
      "Epoch 24000, loss 1.312\n",
      "Epoch 25000, loss 1.312\n",
      "Epoch 26000, loss 1.312\n",
      "Epoch 27000, loss 1.312\n",
      "Epoch 28000, loss 1.312\n",
      "Epoch 29000, loss 1.312\n",
      "Epoch 30000, loss 1.312\n",
      "Epoch 31000, loss 1.312\n",
      "Epoch 32000, loss 1.312\n",
      "Epoch 33000, loss 1.312\n",
      "Epoch 34000, loss 1.312\n",
      "Epoch 35000, loss 1.312\n",
      "Epoch 36000, loss 1.312\n",
      "Epoch 37000, loss 1.312\n",
      "Epoch 38000, loss 1.312\n",
      "Epoch 39000, loss 1.312\n",
      "Epoch 40000, loss 1.312\n",
      "==================================================\n",
      "Train err of final w:  8.018514153470333\n"
     ]
    }
   ],
   "source": [
    "W = np.random.randn(train_X.shape[1], 1)\n",
    "losses=[]\n",
    "num_epochs=40000\n",
    "learning_rate=0.01\n",
    "W=train(W,train_X, train_y, learning_rate, num_epochs , losses)\n",
    "x_preds=predict(W ,train_X)\n",
    "train_err = np.mean(x_preds != train_y) * 100\n",
    "print ('=' * 50)\n",
    "print ('Train err of final w: ', train_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Evaluate our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95     41413\n",
      "           1       0.59      0.92      0.72      5254\n",
      "\n",
      "    accuracy                           0.92     46667\n",
      "   macro avg       0.79      0.92      0.84     46667\n",
      "weighted avg       0.94      0.92      0.93     46667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = predict(W, train_X)\n",
    "print(classification_report(train_y, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.92      0.95     20710\n",
      "           1       0.59      0.93      0.72      2623\n",
      "\n",
      "    accuracy                           0.92     23333\n",
      "   macro avg       0.79      0.92      0.84     23333\n",
      "weighted avg       0.95      0.92      0.93     23333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = predict(W, test_X)\n",
    "print(classification_report(test_y, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Comment on the result**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions:\n",
    "\n",
    "1. **High Precision for Class 0**: \n",
    "   - Precision for class 0 is high (0.97), indicating that when the model predicts a sample as class 0, it is very likely to be correct. However, this needs to be balanced against other metrics.\n",
    "2. **Lower Recall for Class 0**: \n",
    "   - The recall for class 0 is relatively lower (0.77), implying that the model misses about 23% of actual class 0 instances. \n",
    "3. **Low Precision but High Recall for Class 1**: \n",
    "   - For class 1, the precision is low (around 0.31-0.32), meaning that many of the instances predicted as class 1 are false positives. However, the recall is high (0.82-0.84), indicating the model is good at capturing most of the actual class 1 instances.\n",
    "4. **Overall Accuracy**: \n",
    "   - The accuracy is around 77-78%, which is decent but can be misleading due to class imbalance. The model is better at identifying class 1 instances than correctly labeling class 0 instances.\n",
    "5. **Macro and Weighted Averages**: \n",
    "   - The macro average, which treats both classes equally, is lower than the weighted average, which gives more weight to the majority class (class 0). This difference is due to the class imbalance and reflects in the f1-scores.\n",
    "6. **Imbalance Between Classes**: \n",
    "   - The significant difference in support numbers between classes (more than 7 times) indicates a class imbalance problem. This imbalance likely contributes to the disparities in precision and recall between the two classes.\n",
    "\n",
    "\n",
    "### Causes of imbalance:\n",
    "\n",
    "1. **Data imbalance**:\n",
    "   - The significant difference in sample sizes between the two classes (41410 vs. 5257 in the training set, 20713 vs. 2620 in the test set) is a primary cause.\n",
    "   - The model might have \"learned\" that predicting all samples as class 0 yields a high success rate, thus becoming biased towards the majority class.\n",
    "2. **Model capability**:\n",
    "   - Logistic Regression, being a linear model, might not suit the complex nature of the data, or fail to capture non-linear relationships between features and the target class.\n",
    "\n",
    "### Solutions:\n",
    "\n",
    "1. **Reconsider data handling**:\n",
    "   - Apply resampling techniques such as oversampling for the minority class or undersampling for the majority class.\n",
    "   - Use the SMOTE method to generate synthetic data for the minority class.\n",
    "2. **Change evaluation metrics**:\n",
    "   - Use metrics like F1-score, Precision-Recall AUC, or ROC AUC instead of relying solely on accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1. Nevil, S. (2023, March 31). How to calculate Z-Score and its meaning. Investopedia. \n",
    "  - https://www.investopedia.com/terms/z/zscore.asp\n",
    "2. StatQuest with Josh Starmer. (2018, March 5). StatQuest: Logistic Regression [Video]. \n",
    "  - YouTube. https://www.youtube.com/watch?v=yIYKR4sgzI8\n",
    "3. StatQuest with Josh Starmer. (2017, July 31). Maximum Likelihood, clearly explained!!! [Video]. \n",
    "  - YouTube. https://www.youtube.com/watch?v=XepXtl9YKwc \n",
    "4. Leung, K. (2022, October 4). Assumptions of logistic regression, clearly explained. \n",
    "  - Medium site. https://towardsdatascience.com/assumptions-of-logistic-regression-clearly-explained-44d85a22b290\n",
    "5. Satpathy, S. (2023, November 17). SMOTE for Imbalanced Classification with Python. Analytics Vidhya. \n",
    "  - https://www.analyticsvidhya.com/blog/2020/10/overcoming-class-imbalance-using-smote-techniques/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
