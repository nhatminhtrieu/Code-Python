{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b76YngfGGfyD"
   },
   "source": [
    "# Lab04: Decision Tree and Naive Bayes\n",
    "\n",
    "- Student ID: 21127112\n",
    "- Student name: Triệu Nhật Minh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-xZqh-Z7GfyF"
   },
   "source": [
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`) Copy file notebook to this folder, compress and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Decision Tree.\n",
    "- Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "--NRbml7GfyG"
   },
   "source": [
    "### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VhR1GCY5GfyH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-OzYr2SGfyN"
   },
   "source": [
    "### Load Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oX5c3r4uGfyO"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "iris=datasets.load_iris()\n",
    "\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "#split dataset into training data and testing data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "US1KgZBgGfyU"
   },
   "source": [
    "## 1. Decision Tree: Iterative Dichotomiser 3 (ID3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4sQh1ieuGfyV"
   },
   "source": [
    "### 1.1 Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MSjCJR_eGfyV"
   },
   "source": [
    "Expected value of the self-information (entropy):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BZM7fmb0GfyW"
   },
   "source": [
    "$$Entropy=-\\sum_{i}^{n}p_ilog_{2}(p_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WDjtCHd_GfyX"
   },
   "source": [
    "The entropy function gets the smallest value if there is a value of $p_i$ equal to 1, reaches the maximum value if all $ p_i $ are equal. These properties of the entropy function make it is an expression of the disorder, or randomness of a system, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kql-MFq-GfyX"
   },
   "outputs": [],
   "source": [
    "def entropy(counts, n_samples):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "    counts: shape (n_classes): list number of samples in each class\n",
    "    n_samples: number of data samples\n",
    "    \n",
    "    -----------\n",
    "    return entropy \n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    entropy = 0\n",
    "    for i in counts:\n",
    "        if i != 0:\n",
    "            entropy += -i/n_samples*np.log2(i/n_samples)\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AsGJfLhmGfyc"
   },
   "outputs": [],
   "source": [
    "def entropy_of_one_division(division): \n",
    "    \"\"\"\n",
    "    Returns entropy of a divided group of data\n",
    "    Data may have multiple classes\n",
    "    \"\"\"\n",
    "    n_samples = len(division)\n",
    "    n_classes = set(division)\n",
    "    \n",
    "    counts=[]\n",
    "    #count samples in each class then store it to list counts\n",
    "    #TODO:\n",
    "\n",
    "    for i in n_classes:\n",
    "        counts.append(len(division[division==i]))\n",
    "    \n",
    "    return entropy(counts,n_samples),n_samples\n",
    "\n",
    "\n",
    "def get_entropy(y_predict, y):\n",
    "    \"\"\"\n",
    "    Returns entropy of a split\n",
    "    y_predict is the split decision by cutoff, True/Fasle\n",
    "    \"\"\"\n",
    "    n = len(y)\n",
    "    entropy_true, n_true = entropy_of_one_division(y[y_predict]) # left hand side entropy\n",
    "    entropy_false, n_false = entropy_of_one_division(y[~y_predict]) # right hand side entropy\n",
    "    # overall entropy\n",
    "    #TODO s=?\n",
    "    s = n_true/n*entropy_true + n_false/n*entropy_false\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dWhnKZm9Gfyi"
   },
   "source": [
    "The information gain of classifying information set D by attribute A:\n",
    "$$ Gain(A)=Entrophy(D)-Entrophy_{A}(D)$$\n",
    "\n",
    "At each node in ID3, an attribute is chosen if its information gain is highest compare to others.\n",
    "\n",
    "All attributes of the Iris set are represented by continuous values. Therefore we need to represent them with discrete values. The simple way is to use a `cutoff` threshold to separate values of the data on each attribute into two part:` <cutoff` and `> = cutoff`.\n",
    "\n",
    "To find the best `cutoff` for an attribute, we replace` cutoff` with its values then compute the entropy, best `cutoff` achieved when value of entropy is smallest  $ \\left (\\arg \\min Entrophy_ {A} (D) \\right) $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tTKwaSw-Gfyj"
   },
   "source": [
    "### 1.2 Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xp6omaz2Gfyj"
   },
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, tree=None, depth=0):\n",
    "        '''Parameters:\n",
    "        -----------------\n",
    "        tree: decision tree\n",
    "        depth: depth of decision tree after training'''\n",
    "        \n",
    "        self.depth = depth\n",
    "        self.tree=tree\n",
    "    def fit(self, X, y, node={}, depth=0):\n",
    "        '''Parameter:\n",
    "        -----------------\n",
    "        X: training data\n",
    "        y: label of training data\n",
    "        ------------------\n",
    "        return: node \n",
    "        \n",
    "        node: each node represented by cutoff value and column index, value and children.\n",
    "         - cutoff value is thresold where you divide your attribute\n",
    "         - column index is your data attribute index\n",
    "         - value of node is mean value of label indexes, \n",
    "           if a node is leaf all data samples will have same label\n",
    "        \n",
    "        Note that: we divide each attribute into 2 part => each node will have 2 children: left, right.\n",
    "        '''\n",
    "        \n",
    "        #Stop conditions\n",
    "        \n",
    "        #if all value of y are the same \n",
    "        if np.all(y==y[0]):\n",
    "            return {'val':y[0]}\n",
    "\n",
    "        else: \n",
    "            col_idx, cutoff, entropy = self.find_best_split_of_all(X, y)    # find one split given an information gain \n",
    "            y_left = y[X[:, col_idx] < cutoff]\n",
    "            y_right = y[X[:, col_idx] >= cutoff]\n",
    "            node = {'index_col':col_idx,\n",
    "                        'cutoff':cutoff,\n",
    "                   'val':np.mean(y)}\n",
    "            node['left'] = self.fit(X[X[:, col_idx] < cutoff], y_left, {}, depth+1)\n",
    "            node['right'] = self.fit(X[X[:, col_idx] >= cutoff], y_right, {}, depth+1)\n",
    "            self.depth += 1 \n",
    "            self.tree = node\n",
    "            return node\n",
    "    \n",
    "    def find_best_split_of_all(self, X, y):\n",
    "        col_idx = None\n",
    "        min_entropy = 1\n",
    "        cutoff = None\n",
    "        for i, col_data in enumerate(X.T):\n",
    "            entropy, cur_cutoff = self.find_best_split(col_data, y)\n",
    "            if entropy == 0:                   #best entropy\n",
    "                return i, cur_cutoff, entropy\n",
    "            elif entropy <= min_entropy:\n",
    "                min_entropy = entropy\n",
    "                col_idx = i\n",
    "                cutoff = cur_cutoff\n",
    "               \n",
    "        return col_idx, cutoff, min_entropy\n",
    "    \n",
    "    def find_best_split(self, col_data, y):\n",
    "        ''' Parameters:\n",
    "        -------------\n",
    "        col_data: data samples in column'''\n",
    "         \n",
    "        min_entropy = 10\n",
    "\n",
    "        #Loop through col_data find cutoff where entropy is minimum\n",
    "        \n",
    "        for value in set(col_data):\n",
    "            y_predict = col_data < value\n",
    "            my_entropy = get_entropy(y_predict, y)\n",
    "            #TODO\n",
    "            #min entropy=?, cutoff=?\n",
    "            min_entropy = min(min_entropy, my_entropy)\n",
    "            cutoff = value    \n",
    "\n",
    "        return min_entropy, cutoff\n",
    "                                               \n",
    "    def predict(self, X):\n",
    "        tree = self.tree\n",
    "        pred = np.zeros(shape=len(X))\n",
    "        for i, c in enumerate(X):\n",
    "            pred[i] = self._predict(c)\n",
    "        return pred\n",
    "    \n",
    "    def _predict(self, row):\n",
    "        cur_layer = self.tree\n",
    "        while cur_layer.get('cutoff'):\n",
    "            if row[cur_layer['index_col']] < cur_layer['cutoff']:\n",
    "                cur_layer = cur_layer['left']\n",
    "            else:\n",
    "                cur_layer = cur_layer['right']\n",
    "        else:\n",
    "            return cur_layer.get('val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_OsIHd-Gfyq"
   },
   "source": [
    "### 1.3 Classification on Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BNgHip1dGfyr",
    "outputId": "12173b62-c713-4ad2-ca10-81d8addc7112"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\MINH\\Downloads\\Coding\\Code-Python\\Notebooks\\Intro2ML\\Lab04\\Lab04-DecisionTree_BayesTheorem.ipynb Cell 18\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m DecisionTreeClassifier()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m tree \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m pred\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mpredict(X_train)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy of your decision tree model on training data:\u001b[39m\u001b[39m'\u001b[39m, accuracy_score(y_train,pred))\n",
      "\u001b[1;32mc:\\Users\\MINH\\Downloads\\Coding\\Code-Python\\Notebooks\\Intro2ML\\Lab04\\Lab04-DecisionTree_BayesTheorem.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y_right \u001b[39m=\u001b[39m y[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m node \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m'\u001b[39m:col_idx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m'\u001b[39m:cutoff,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:np\u001b[39m.\u001b[39mmean(y)}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X[X[:, col_idx] \u001b[39m<\u001b[39;49m cutoff], y_left, {}, depth\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff], y_right, {}, depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n",
      "\u001b[1;32mc:\\Users\\MINH\\Downloads\\Coding\\Code-Python\\Notebooks\\Intro2ML\\Lab04\\Lab04-DecisionTree_BayesTheorem.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y_right \u001b[39m=\u001b[39m y[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m node \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m'\u001b[39m:col_idx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m'\u001b[39m:cutoff,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:np\u001b[39m.\u001b[39mmean(y)}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X[X[:, col_idx] \u001b[39m<\u001b[39;49m cutoff], y_left, {}, depth\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff], y_right, {}, depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n",
      "\u001b[1;32mc:\\Users\\MINH\\Downloads\\Coding\\Code-Python\\Notebooks\\Intro2ML\\Lab04\\Lab04-DecisionTree_BayesTheorem.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y_right \u001b[39m=\u001b[39m y[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m node \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m'\u001b[39m:col_idx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m'\u001b[39m:cutoff,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:np\u001b[39m.\u001b[39mmean(y)}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X[X[:, col_idx] \u001b[39m<\u001b[39;49m cutoff], y_left, {}, depth\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff], y_right, {}, depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n",
      "\u001b[1;32mc:\\Users\\MINH\\Downloads\\Coding\\Code-Python\\Notebooks\\Intro2ML\\Lab04\\Lab04-DecisionTree_BayesTheorem.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m node \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m'\u001b[39m:col_idx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m'\u001b[39m:cutoff,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:np\u001b[39m.\u001b[39mmean(y)}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X[X[:, col_idx] \u001b[39m<\u001b[39m cutoff], y_left, {}, depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X[X[:, col_idx] \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m cutoff], y_right, {}, depth\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtree \u001b[39m=\u001b[39m node\n",
      "\u001b[1;32mc:\\Users\\MINH\\Downloads\\Coding\\Code-Python\\Notebooks\\Intro2ML\\Lab04\\Lab04-DecisionTree_BayesTheorem.ipynb Cell 18\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m y_right \u001b[39m=\u001b[39m y[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m node \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mindex_col\u001b[39m\u001b[39m'\u001b[39m:col_idx,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mcutoff\u001b[39m\u001b[39m'\u001b[39m:cutoff,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:np\u001b[39m.\u001b[39mmean(y)}\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mleft\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X[X[:, col_idx] \u001b[39m<\u001b[39;49m cutoff], y_left, {}, depth\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m node[\u001b[39m'\u001b[39m\u001b[39mright\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X[X[:, col_idx] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m cutoff], y_right, {}, depth\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdepth \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \n",
      "\u001b[1;32mc:\\Users\\MINH\\Downloads\\Coding\\Code-Python\\Notebooks\\Intro2ML\\Lab04\\Lab04-DecisionTree_BayesTheorem.ipynb Cell 18\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Parameter:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m-----------------\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mX: training data\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mNote that: we divide each attribute into 2 part => each node will have 2 children: left, right.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m#Stop conditions\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m#if all value of y are the same \u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mall(y\u001b[39m==\u001b[39my[\u001b[39m0\u001b[39;49m]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m:y[\u001b[39m0\u001b[39m]}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/MINH/Downloads/Coding/Code-Python/Notebooks/Intro2ML/Lab04/Lab04-DecisionTree_BayesTheorem.ipynb#X23sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39melse\u001b[39;00m: \n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "tree = model.fit(X_train, y_train)\n",
    "pred=model.predict(X_train)\n",
    "print('Accuracy of your decision tree model on training data:', accuracy_score(y_train,pred))\n",
    "pred=model.predict(X_test)\n",
    "print('Accuracy of your decision tree model:', accuracy_score(y_test,pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2rXS4sPCGfyz"
   },
   "source": [
    "## 2. Bayes Theorem\n",
    "\n",
    "Bayes formulation\n",
    "$$\\begin{equation}\n",
    "P\\left(A|B\\right)= \\dfrac{P\\left(B|A\\right)P\\left(A\\right)}{P\\left(B\\right)}\n",
    "\\end{equation}$$\n",
    "\n",
    "If $B$ is our data $\\mathcal{D}$, $A$ and $w$ are parameters we need to estimate:\n",
    "\n",
    "$$ \\begin{align}\n",
    "    \\underbrace{P(w|\\mathcal{D})}_{Posterior}= \\dfrac{1}{\\underbrace{P(\\mathcal{D})}_{Normalization}} \\overbrace{P(\\mathcal{D}|w)}^{\\text{Likelihood}} \\overbrace{P(w)}^{Prior}\n",
    "    \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zuPXhYHzGfy1"
   },
   "source": [
    "#### Naive Bayes\n",
    "To make it simple, it is often assumed that the components of the $D$ random variable (or the features of the $D$ data) are independent with each other, if $w$ is known. It mean:\n",
    "\n",
    "$$P(\\mathcal{D}|w)=\\prod _{i=1}^{d}P(x_i|w)$$\n",
    "\n",
    "- $d$: number of features\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1m4AZLwgGfy3"
   },
   "source": [
    "### 2.1. Probability Density Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fA3arZy8Gfy4"
   },
   "outputs": [],
   "source": [
    "class pdf:\n",
    "    def __init__(self,hist=None):\n",
    "        '''\n",
    "        A probability density function represented by a histogram\n",
    "        \n",
    "        hist: shape (n,1), n: number of hypotheses\n",
    "        hypo: hypothesis (simply understand as label)\n",
    "        ------------------\n",
    "        hist[hypo]=P(hypo)\n",
    "        '''\n",
    "        self.hist = hist\n",
    "        \n",
    "    #virtual function\n",
    "    def likelihood(self, data, hypo):\n",
    "        '''Paramters:\n",
    "        data: new data record \n",
    "        hypo: hypothesis (simply understand as label)\n",
    "        ---------\n",
    "        return P(data/hypo)\n",
    "        ''' \n",
    "        raise Exception()\n",
    "            \n",
    "    #update histogram for new data \n",
    "    def update(self, data):\n",
    "        ''' \n",
    "        P(hypo/data)=P(data/hypo)*P(hypo)*(1/P(data))\n",
    "        '''\n",
    "        \n",
    "        #Likelihood * Prior \n",
    "        #TODO\n",
    "        for hypo in self.hist.keys():\n",
    "            #self.hist[hypo]=?\n",
    "            self.hist[hypo] = self.likelihood(data, hypo)*self.hist[hypo]\n",
    "            \n",
    "        #Normalization\n",
    "        \n",
    "        #TODO: s=P(data)\n",
    "        #s=?\n",
    "        s = sum(self.hist.values())\n",
    "        \n",
    "        for hypo in self.hist.keys():\n",
    "            self.hist[hypo] = self.hist[hypo]/s\n",
    "        \n",
    "    def plot_pdf(self):\n",
    "        #plot Histogram\n",
    "        #TODO\n",
    "        plt.bar(self.hist.keys(), self.hist.values())\n",
    "        plt.xlabel('Hypothesis')\n",
    "      \n",
    "    \n",
    "    def maxHypo(self):\n",
    "        #find the hypothesis with maximum probability from hist\n",
    "        #TODO\n",
    "        \n",
    "        return max(self.hist, key=self.hist.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x89nTrIEGfy7"
   },
   "source": [
    "### 2.2 Classification on Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y9moO4N2Gfy8"
   },
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rhLm2pD-Gfy-"
   },
   "source": [
    "- Naive Bayes can be extended to use on continuous data, most commonly by using a normal distribution (Gaussian distribution).\n",
    "\n",
    "- This extension called Gaussian Naive Bayes. Other functions can be used to estimate data distribution, but Gauss (or the normal distribution) is the easiest to work with since we only need to estimate the mean and standard deviation from the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VDbWOYQ-GfzA"
   },
   "source": [
    "#### Define Gauss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TmlbwFHPGfzA"
   },
   "source": [
    "$$ f\\left(x;\\mu,\\sigma \\right)= \\dfrac{1}{\\sigma \\sqrt{2\\pi}} \n",
    "\\exp \\left({-\\dfrac{\\left(x-\\mu\\right)^2}{2 \\sigma^2}}\\right) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-OpY89tkGfzB"
   },
   "outputs": [],
   "source": [
    "def Gauss(std,mean,x):\n",
    "    #Compute the Gaussian probability distribution function for x\n",
    "    #TODO \n",
    "    return 1/(std*np.sqrt(2*np.pi))*np.exp(-1/2*((x-mean)/std)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RIutnepWGfzH"
   },
   "outputs": [],
   "source": [
    "class NBGaussian(pdf):\n",
    "    def __init__(self, hist=None, std=None, mean=None):\n",
    "        '''Parameters:\n",
    "        \n",
    "        '''\n",
    "        pdf.__init__(self, hist)\n",
    "        self.std=std\n",
    "        self.mean=mean\n",
    "    def likelihood(self,data, hypo):\n",
    "        '''\n",
    "        Returns: res=P(data/hypo)\n",
    "        -----------------\n",
    "        Naive bayes:\n",
    "            Atributes are assumed to be conditionally independent given the class value.\n",
    "        '''\n",
    "    \n",
    "        std=self.std[hypo]\n",
    "        mean=self.mean[hypo]\n",
    "        res=1\n",
    "        #TODO\n",
    "        #res=res*P(x1/hypo)*P(x2/hypo)...\n",
    "        for i in range(len(data)):\n",
    "            res *= Gauss(std[i], mean[i], data[i])\n",
    "\n",
    "        return res \n",
    "    def fit(self, X,y):\n",
    "        \"\"\"Parameters:\n",
    "        X: training data\n",
    "        y: labels of training data\n",
    "        \"\"\"\n",
    "        n=len(X)\n",
    "        #number of iris species\n",
    "        #TODO\n",
    "        #n_species=???\n",
    "        n_species = len(set(y))\n",
    "        \n",
    "        hist={}\n",
    "        mean={}\n",
    "        std={}\n",
    "        \n",
    "        #separate  dataset into rows by class\n",
    "        for hypo in range(0,n_species):\n",
    "            #rows have hypo label\n",
    "            #TODO rows=\n",
    "            rows = X[y==hypo]\n",
    "        \n",
    "            #histogram for each hypo\n",
    "            #TODO probability=?\n",
    "            probability = len(rows)/n\n",
    "            \n",
    "            hist[hypo]=probability\n",
    "            \n",
    "            #Each hypothesis represented by its mean and standard derivation\n",
    "            '''mean and standard derivation should be calculated for each column (or each attribute)'''\n",
    "            #TODO mean[hypo]=?, std[hypo]=?\n",
    "            mean[hypo] = np.mean(rows, axis=0)\n",
    "            std[hypo] = np.std(rows, axis=0)\n",
    "         \n",
    "        self.mean=mean\n",
    "        self.std=std\n",
    "        self.hist=hist\n",
    "   \n",
    "    def _predict(self, data, plot=False):\n",
    "        \"\"\"\n",
    "        Predict label for only 1 data sample\n",
    "        ------------\n",
    "        Parameters:\n",
    "        data: data sample\n",
    "        plot: True: draw histogram after update new record\n",
    "        -----------\n",
    "        return: label of data\n",
    "        \"\"\"\n",
    "        model=NBGaussian(hist=self.hist.copy(),std=self.std.copy(), mean=self.mean.copy())\n",
    "        model.update(data)\n",
    "        if (plot): model.plot_pdf()\n",
    "        return model.maxHypo()\n",
    "    \n",
    "    def predict(self, data):\n",
    "        \"\"\"Parameters:\n",
    "        Data: test data\n",
    "        ----------\n",
    "        return labels of test data\"\"\"\n",
    "        \n",
    "        pred=[]\n",
    "        for x in data:\n",
    "            pred.append(self._predict(x))\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Gmv2qqxGfzM"
   },
   "source": [
    "#### Show histogram of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BkjhuGkLGfzN",
    "outputId": "0cacea13-c482-4706-f759-2da97552fe4d"
   },
   "outputs": [],
   "source": [
    "model_1=NBGaussian()\n",
    "model_1.fit(X_train, y_train)\n",
    "model_1.plot_pdf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itiPerhxGfzW"
   },
   "source": [
    "#### Test wih 1 data record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ev9UTlb6GfzX",
    "outputId": "ad1d31ca-6cf9-4081-a0fe-ff7c65963db3"
   },
   "outputs": [],
   "source": [
    "#label of y_test[10]\n",
    "print('Label of X_test[10]: ', y_test[10])\n",
    "#update model and show histogram with X_test[10]:\n",
    "\n",
    "print('Our histogram after update X_test[10]: ')\n",
    "model_1._predict(X_test[10],plot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CAhcGUTgGfzd"
   },
   "source": [
    "#### Evaluate your Gaussian Naive Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S5Kvk-rUGfze",
    "outputId": "9ba3c648-275d-446f-d3f2-b0256a07a241"
   },
   "outputs": [],
   "source": [
    "pred=model_1.predict(X_test)\n",
    "print('Accuracy of your Gaussian Naive Bayes model:', accuracy_score(y_test,pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gJaWYqt5Jvmp"
   },
   "source": [
    "**TODO**: F1, Recall and Precision report"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab04-DecisionTree&BayesTheorem.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
