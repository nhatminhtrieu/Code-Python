{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bài tập 2\n",
    "\n",
    "\n",
    "Triệu Nhật Minh - 21127112\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - 2: Hoeffding Inequality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.037717"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_coins = 1000\n",
    "n_flips = 10\n",
    "mu = 0.5\n",
    "n_exps = 100000\n",
    "\n",
    "result = np.random.binomial(n_flips, mu, (n_exps, n_coins)) / n_flips\n",
    "\n",
    "v_1 = result[:, 0]\n",
    "v_rand = result[np.arange(n_exps), np.random.choice(n_coins, size=n_exps)]\n",
    "v_min = np.min(result, axis=1)\n",
    "\n",
    "np.mean(v_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First coin is Hoeffding? True\n",
      "Random coin is Hoeffding? True\n",
      "Min coin is Hoeffding? False\n"
     ]
    }
   ],
   "source": [
    "def is_hoeffding_coin(results):\n",
    "    # Calculate the sum of the results\n",
    "    sum_results = sum(results)\n",
    "\n",
    "    # Calculate the expected value of the results\n",
    "    expected_value = len(results) * 0.5  # Assuming a fair coin\n",
    "\n",
    "    # Calculate the deviation\n",
    "    deviation = abs(sum_results - expected_value)\n",
    "\n",
    "    # Check if the deviation is less than or equal to the bound given by the Hoeffding Inequality\n",
    "    # This is a simplified version of the inequality and might not be accurate for your use case\n",
    "    return deviation <= np.sqrt(len(results) * np.log(2) / 2)\n",
    "\n",
    "print(\"First coin is Hoeffding?\", is_hoeffding_coin(v_1))\n",
    "print(\"Random coin is Hoeffding?\", is_hoeffding_coin(v_rand))\n",
    "print(\"Min coin is Hoeffding?\", is_hoeffding_coin(v_min))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - 4: Error and Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbb{P}(y|\\text{x}) = \\begin{cases} \\lambda & y = f(x) \\\\ 1 - \\lambda & y \\neq f(x) \\end{cases}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of binary classification, where the output `y` can take on two possible values (let's say -1 and 1), there are typically only two error cases to consider:\n",
    "\n",
    "1. The classifier (or hypothesis) `h` predicts `y = f(x)` (a correct prediction) with probability $1 - \\mu$, but the actual `y` is not equal to `f(x)` with the probability $1 - \\lambda$. This situation is referred to as a ***false accept***. The probability of this happening is $(1 - \\lambda) * (1 - \\mu)$\n",
    "\n",
    "2. The classifier `h` predicts `y ≠ f(x)` (incorrect prediction) with probability $\\mu$, but the actual `y` is equal to `f(x)` with the probability $\\lambda$. This situation is referred to as a ***false reject***. The probability of this happening is $\\lambda * \\mu$\n",
    "\n",
    "These two cases cover all the possible error scenarios for a binary classifier. Therefore, the probability of error is the sum of the probabilities of these two cases:\n",
    "\n",
    "$$\\mathbb{P}(\\text{error}) = (1 - \\lambda) * (1 - \\mu) + \\lambda * \\mu$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "The correct answer is [e] $(1 - \\lambda) * (1 - \\mu) + \\lambda * \\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the probability of error that $h$ makes in approximating $y$:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\mathbb{P}(\\text{error}) &= (1 - \\lambda) * (1 - \\mu) + \\lambda * \\mu \\\\\n",
    "&= 1 - \\mu - \\lambda + 2\\mu\\lambda\\\\\n",
    "&= \\mu(2\\lambda - 1) + 1 - \\lambda\n",
    "\\end{align*}$$\n",
    "\n",
    "if $\\lambda = 0.5$, then $\\mathbb{P}(\\text{error}) = \\frac{1}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correct answer is [b] 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - 7: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm phát sinh ra `target_w`, véc-tơ tham số của $f$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_target_w(): # Code from HW1\n",
    "    \"\"\"\n",
    "    Generates target_w from two random, uniformly distributed points in [-1, 1] x [-1, 1].\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    target_w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of f.\n",
    "    \"\"\"\n",
    "    # Generate two points from a uniform distribution over [-1, 1]x[-1, 1]\n",
    "    p1 = np.random.uniform(-1, 1, 2)\n",
    "    p2 = np.random.uniform(-1, 1, 2)\n",
    "    # Compute the target W from these two points\n",
    "    target_w = np.array([p1[1] * p2[0] - p1[0] * p2[1], p2[1] - p1[1], p1[0] - p2[0]]).reshape((-1, 1))\n",
    "    \n",
    "    return target_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm phát sinh ra tập dữ liệu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(N, target_w): # Code from HW1\n",
    "    \"\"\"\n",
    "    Generates a data set by generating random inputs and then using target_w to generate the \n",
    "    corresponding outputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of examples.\n",
    "    target_w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of f.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : numpy array, shape (N, 3)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones.\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.        \n",
    "    \"\"\"\n",
    "    X = np.random.uniform(-1, 1, (N, 2))\n",
    "    X = np.hstack((np.ones((N, 1)), X)) # Add 'ones' column\n",
    "    Y = np.sign(np.dot(X, target_w))\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm chạy Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_LinearRegression(X, Y):\n",
    "    \"\"\"\n",
    "    Runs the Linear Regression algorithm on X, Y.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, 3)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones.\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of g.\n",
    "    \"\"\"\n",
    "    # Calculate w using the pseudo-inverse\n",
    "    w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), Y)\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_LR():\n",
    "    \"\"\"\n",
    "    Returns\n",
    "    -------\n",
    "    w: numpy array, shape (3, 1)\n",
    "        The vector of parameters of g found by Linear Regression.\n",
    "    \"\"\"\n",
    "    num_runs = 1000 # Number of experiments to run\n",
    "\n",
    "    avg_Ein = 0.0 # The average in-sample error of g\n",
    "    \n",
    "    avg_Eout = 0.0 # The average out-of-sample error of g\n",
    "    \n",
    "    # Number of in-sample points\n",
    "    n_in = 100\n",
    "    \n",
    "    # Number of out-of-sample points\n",
    "    n_out = 1000\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        # Generate target_w\n",
    "        target_w = generate_target_w()\n",
    "        # Generate training set\n",
    "        X_in, y_in = generate_data(n_in, target_w)\n",
    "        # Generate out-of-sample dataset\n",
    "        X_out, y_out = generate_data(n_out, target_w)\n",
    "        \n",
    "        # Run Linear Regression to pick g\n",
    "        w = run_LinearRegression(X_in, y_in)\n",
    "        \n",
    "        # Predict on training set with found w\n",
    "        predictions_in = np.dot(X_in, w)\n",
    "        # Predict on out-of-sample set with found w\n",
    "        predictions_out = np.dot(X_out, w)\n",
    "        \n",
    "        # Compute binary error between y_in/ y_out - correct output & predictions\n",
    "        Ein = np.mean(y_in != np.sign(predictions_in))\n",
    "        Eout = np.mean(y_out != np.sign(predictions_out))\n",
    "        \n",
    "        # Update average error\n",
    "        avg_Ein += (Ein * 1.0 / num_runs)\n",
    "        avg_Eout += (Eout * 1.0 / num_runs)\n",
    "    \n",
    "    print(\"Average Ein: \", avg_Ein)\n",
    "    print(\"Average Eout: \", avg_Eout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Ein:  0.038049999999999834\n",
      "Average Eout:  0.048591999999999996\n"
     ]
    }
   ],
   "source": [
    "main_LR()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[5] The correct answer is [c] 0.01\n",
    "\n",
    "[6] The correct answer is [c] 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm chạy PLA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_PLA(X, Y): # Code from HW1, modified w to be the output of Linear Regression\n",
    "    \"\"\"\n",
    "    Runs PLA.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, 3)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of \n",
    "        this matrix is all ones.\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.\n",
    "    w : numpy array, shape (3, 1) \n",
    "        The vector of parameters of g found by Linear Regression.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    num_iterations : int\n",
    "        The number of iterations PLA takes to converge.\n",
    "    \"\"\"\n",
    "    w = run_LinearRegression(X, Y)\n",
    "    num_iterations = 0\n",
    "    \n",
    "    while True:\n",
    "        misclassified_indices = np.where(np.sign(np.dot(X, w)) != Y)[0] # Find misclassified indices\n",
    "        if misclassified_indices.size == 0:\n",
    "            break\n",
    "        i = np.random.choice(misclassified_indices) # Stochastic Gradient Descent\n",
    "        w += Y[i] * X[i].reshape(X.shape[1], 1) # Update w\n",
    "        num_iterations += 1  \n",
    "    \n",
    "    return num_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_PLA(N):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of training examples.\n",
    "    \"\"\"\n",
    "    num_runs = 1000\n",
    "    # The average number of iterations PLA takes to converge\n",
    "    avg_num_iterations = 0.0\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        # Generate target_w\n",
    "        target_w = generate_target_w()\n",
    "        \n",
    "        # Generate training set\n",
    "        X, Y = generate_data(N, target_w)\n",
    "        \n",
    "        # Run PLA to completely separates all the in-sample points\n",
    "        num_iterations = run_PLA(X, Y)\n",
    "        \n",
    "        # Update average num_iterations\n",
    "        avg_num_iterations += (num_iterations * 1.0 / num_runs)\n",
    "    \n",
    "    # Print results\n",
    "    print('avg_num_iterations = %f' % (avg_num_iterations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg_num_iterations = 3.965000\n"
     ]
    }
   ],
   "source": [
    "main_PLA(N=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[7] The correct answer is [a] 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - 10: Nonlinear Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x_1, x_2) = \\text{sign}(x_1^2 + x_2^2 - 0.6)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_with_noise(N, noise):\n",
    "    \"\"\"\n",
    "    Generates a data set with a given number of examples and a given noise level.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of examples.\n",
    "    noise : float\n",
    "        The probability that a label is flipped.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X : numpy array, shape (N, 3)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of this matrix is all ones.\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.        \n",
    "    \"\"\"\n",
    "    X = np.random.uniform(-1, 1, (N, 2))\n",
    "    X = np.hstack((np.ones((N, 1)), X)) # Add 'ones' column\n",
    "    Y = np.sign(X[:, 1]**2 + X[:, 2]**2 - 0.6)\n",
    "    \n",
    "    # Flip the sign of the output for a randomly selected noise subset of the points\n",
    "    num_flips = int(noise * N)\n",
    "    flip_indices = np.random.choice(N, num_flips, replace=False)\n",
    "    Y[flip_indices] = -Y[flip_indices]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_NT_8(N):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of training examples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    avg_Ein : float\n",
    "        The average in-sample error of g.\n",
    "    \"\"\"\n",
    "    num_runs = 1000 # Number of experiments to run\n",
    "    avg_Ein = 0.0 # The average in-sample error of g\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        # Generate training set\n",
    "        X, Y = generate_data_with_noise(N, noise=0.1)\n",
    "        \n",
    "        # Run Linear Regression to pick g\n",
    "        w = run_LinearRegression(X, Y)\n",
    "        \n",
    "        # Predict on training set with found w\n",
    "        predictions = np.dot(X, w)\n",
    "        \n",
    "        # Compute binary error between y - correct output & predictions\n",
    "        Ein = np.mean(Y != np.sign(predictions))\n",
    "        \n",
    "        # Update average error\n",
    "        avg_Ein += (Ein * 1.0 / num_runs)\n",
    "        \n",
    "    print(\"Average Ein: \", avg_Ein)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Ein:  0.5074069999999996\n"
     ]
    }
   ],
   "source": [
    "main_NT_8(N=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[8] The correct answer is [d] 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hàm chuyển đổi vector đặc trưng sang vector đặc trưng phi tuyến\n",
    "\n",
    "$$z = (1, x_1, x_2, x_1x_2, x_1^2, x_2^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_non_linear(X, Y):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, 3)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); the first column of this matrix is all ones.\n",
    "    Y : numpy array, shape (N, 1)\n",
    "        The vector of outputs.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Z : numpy array, shape (N, 6)\n",
    "        The transformed matrix of input vectors.\n",
    "    \"\"\"\n",
    "    # Transform X into Z\n",
    "    Z = np.hstack((X, (X[:, 1] * X[:, 2]).reshape(-1, 1), (X[:, 1]**2).reshape(-1, 1), (X[:, 2]**2).reshape(-1, 1)))    \n",
    "    \n",
    "    return Z, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_NT_9(N):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    N : int\n",
    "        The number of training examples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    avg_w : numpy array, shape (6, 1)\n",
    "        The average w found by Linear Regression.\n",
    "    \"\"\"\n",
    "    num_runs = 1000 # Number of experiments to run\n",
    "\n",
    "    avg_w = 0 # The average w found by Linear Regression\n",
    "        \n",
    "    for _ in range(num_runs):\n",
    "        X, Y = generate_data_with_noise(N, noise=0.1)\n",
    "        \n",
    "        # Transform X into Z\n",
    "        Z, Y = transform_non_linear(X, Y)\n",
    "        \n",
    "        # Predict on training set with found w\n",
    "        avg_w += (run_LinearRegression(Z, Y) * 1.0 / num_runs)\n",
    "        \n",
    "    return np.array(avg_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_w = main_NT_9(N=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_hypothesis(avg_w, hypotheses):\n",
    "    \"\"\"\n",
    "    Finds the hypothesis that agrees the most with the solution of Linear Regression.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    avg_w : numpy array, shape (6, 1)\n",
    "        The average w found by Linear Regression.\n",
    "    hypotheses : list of numpy arrays\n",
    "        The list of hypotheses to choose from.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    closest_hypothesis : numpy array\n",
    "        The hypothesis that agrees the most with the solution of Linear Regression.\n",
    "    \"\"\"\n",
    "    closest_hypothesis = None # The hypothesis that agrees the most with the solution of Linear Regression\n",
    "    \n",
    "    min_distance = 0.0 # The distance between the solution of Linear Regression and the closest hypothesis\n",
    "    \n",
    "    for _ in range(len(hypotheses)):\n",
    "        # Calculate the distance between avg_w and the current hypothesis\n",
    "        distance = np.linalg.norm(avg_w - hypotheses[_])\n",
    "        \n",
    "        # Update closest_hypothesis and min_distance\n",
    "        if closest_hypothesis is None or distance < min_distance:\n",
    "            closest_hypothesis = hypotheses[_]\n",
    "            min_distance = distance\n",
    "    \n",
    "    return closest_hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closest hypothesis:  [-1.   -0.05  0.08  0.13  1.5   1.5 ]\n"
     ]
    }
   ],
   "source": [
    "hypotheses = [\n",
    "    [-1, -0.05, 0.08, 0.13, 1.5, 1.5], # a\n",
    "    [-1, -0.05, 0.08, 0.13, 1.5, 15], # b\n",
    "    [-1, -0.05, 0.08, 0.13, 15, 1.5], # c\n",
    "    [-1, -1.5, 0.08, 0.13, 0.05, 0.05], # d\n",
    "    [-1, -0.05, 0.08, 1.5, 0.15, 0.15] # e\n",
    "]\n",
    "\n",
    "# Convert to numpy arrays\n",
    "\n",
    "hypotheses = [np.array(hypothesis) for hypothesis in hypotheses]\n",
    "\n",
    "closest_hypothesis = find_closest_hypothesis(avg_w, hypotheses)\n",
    "\n",
    "print(\"Closest hypothesis: \", closest_hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[9] The correct answer is [a] $g(x_1, x_2) = \\text{sign}(-1 - 0.05x_1 + 0.08x_2 + 0.13 x_1x_2 + 1.5x_1^2 + 1.5x_2^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_NT_10(avg_w, N):\n",
    "    \"\"\"\n",
    "    Calculates the out-of-sample error of the hypothesis found in the previous problem.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    avg_w : numpy array, shape (6, 1)\n",
    "        The average w found by Linear Regression.\n",
    "    N : int\n",
    "        The number of training examples.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    avg_Eout : float\n",
    "        The average out-of-sample error of the hypothesis found in the previous problem.\n",
    "    \"\"\"\n",
    "    num_runs = 1000 # Number of experiments to run\n",
    "\n",
    "    avg_Eout = 0.0 # The average out-of-sample error of the hypothesis found in the previous problem\n",
    "        \n",
    "    for _ in range(num_runs):\n",
    "        X, Y = generate_data_with_noise(N, noise=0.1)\n",
    "        \n",
    "        # Transform X into Z\n",
    "        Z, Y = transform_non_linear(X, Y)\n",
    "        \n",
    "        # Predict on training set with found w\n",
    "        predictions = np.dot(Z, avg_w)\n",
    "        \n",
    "        # Compute binary error between y - correct output & predictions\n",
    "        Eout = np.mean(Y != np.sign(predictions))\n",
    "        \n",
    "        # Update average error\n",
    "        avg_Eout += (Eout * 1.0 / num_runs)\n",
    "        \n",
    "    print(\"Average Eout: \", avg_Eout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Eout:  0.12275299999999986\n"
     ]
    }
   ],
   "source": [
    "main_NT_10(avg_w = avg_w, N=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[10] The correct answer is [b] 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. [hthoai - GitHub](https://github.com/hthoai/machine-learning/blob/master/homework/hw2/hw2-sol.ipynb)\n",
    "2. [homefish - GitHub](https://github.com/homefish/edX_Learning_From_Data_2017/blob/master/homework_2/homework_2_problem_3_4_Error_and_Noise.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
