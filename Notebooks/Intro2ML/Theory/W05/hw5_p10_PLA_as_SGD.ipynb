{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5\n",
    "\n",
    "## Problem 10\n",
    "\n",
    "In this problem we are asked to choose the error function $e(\\mathbf{w})$ in SGD such that the PLA is implemented.\n",
    "\n",
    "Recall that the weight vector $\\mathbf{w}$ in **PLA** is updated via\n",
    "\n",
    "$\\mathbf{w} \\leftarrow \\mathbf{w} + y \\mathbf{x}$\n",
    "\n",
    "where $(\\mathbf{x}, \\mathbf{y})$ is a misclassified point (see also slide 13 of Lecture 1).\n",
    "\n",
    "_____\n",
    "\n",
    "Let's choose $e(\\mathbf{w}) = -y \\mathbf{w}^T \\mathbf{x}$ for **SGD**. The partial derivatives are\n",
    "\n",
    "$\\frac{\\partial e(\\mathbf{w})}{\\partial w_k} \n",
    "= \\frac{\\partial }{\\partial w_k} (-y \\mathbf{w}^T \\mathbf{x})\n",
    "= -y x_k $\n",
    "\n",
    "with $k \\in \\{0,1,2 \\}$ .\n",
    "\n",
    "So the gradient is:\n",
    "\n",
    "$\\nabla e = [\\frac{\\partial e(\\mathbf{w})}{\\partial w_0},\\frac{\\partial e(\\mathbf{w})}{\\partial w_1}, \\frac{\\partial e(\\mathbf{w})}{\\partial w_2}] \n",
    "          = [-y x_0, -y x_1, -y x_2]\n",
    "          = -y [x_0, x_1, x_2]\n",
    "          = -y \\mathbf{x}$\n",
    "          \n",
    "In SGD we update $\\mathbf{w}$ via \n",
    "\n",
    "$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta \\nabla e(\\mathbf{w})$\n",
    "\n",
    "(see slide 3 and 4 of lecture 10 and slide 23 of lecture 9)\n",
    "\n",
    "If we now insert the error function $e(\\mathbf{w})$ we chose, then we get\n",
    "\n",
    "$\\mathbf{w} \\leftarrow \\mathbf{w} - \\eta (-y \\mathbf{x})$\n",
    "\n",
    "$\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta (y \\mathbf{x})$\n",
    "\n",
    "With $\\eta = 1$ we get\n",
    "\n",
    "$\\mathbf{w} \\leftarrow \\mathbf{w} + y \\mathbf{x}$\n",
    "\n",
    "However, we also have to accomodate for the fact that we only pick misclassified points in PLA. This can be done by choosing \n",
    "\n",
    "$e(\\mathbf{w}) = - \\min(0, y \\mathbf{w}^T \\mathbf{x})$\n",
    "\n",
    "because for correctly classified points the expression $y \\mathbf{w}^T \\mathbf{x}$ is positive, and in that case \n",
    "\n",
    "$e(\\mathbf{w}) = - \\min(0, y \\mathbf{w}^T \\mathbf{x}) = 0$\n",
    "\n",
    "(see slide 12 of lecture 1)\n",
    "\n",
    "So for correctly classified points the gradient is\n",
    "\n",
    "$\\nabla e(\\mathbf{w}) = \\nabla 0 = \\mathbf{0}$\n",
    "\n",
    "which means that correctly classified points do not contribute to a change of $\\mathbf{w}$ during the update.\n",
    "\n",
    "So the correct answer to this problem is **10[e]** $e(\\mathbf{w}) = - \\min(0, y \\mathbf{w}^T \\mathbf{x})$ ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
