{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "\n",
    "## Problem 8\n",
    "\n",
    "Let's visualize the neural network.\n",
    "\n",
    "![neural_network](figures/hw6_p8_neural_network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "We will follow the description of **Backpropagation** in e-Chapter 7, page 14 .\n",
    "\n",
    "## Step 0: Forward propagation\n",
    "\n",
    "We start with Step 0: Run forward propgation, which is described in e-Chapter 7, page 10. The input is a data point $(\\mathbf{x}, y)$. \n",
    "\n",
    "- Calculate the signals $\\mathbf{s}^{(l)} = (W^{(l)})^T \\mathbf{x}^{(l-1)}$ and save the outputs $\\mathbf{x}^{(l)} = \\theta(\\mathbf{s}^{(l)})$ for layers $l = 1, ..., L$ .\n",
    "\n",
    "- For $l = 1$:\n",
    "\n",
    "### Signal $\\mathbf{s}^{(1)}$ and output $\\mathbf{x}^{(1)}$ for layer 1\n",
    "\n",
    "Calculating the signal $\\mathbf{s}^{(1)}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{s}^{(1)}\n",
    "=\n",
    "(W^{(1)})^T \\mathbf{x}^{(0)}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w^{(1)}_{01} & w^{(1)}_{02} & w^{(1)}_{03} \\\\\n",
    "w^{(1)}_{11} & w^{(1)}_{12} & w^{(1)}_{13} \\\\\n",
    "w^{(1)}_{21} & w^{(1)}_{22} & w^{(1)}_{23} \\\\\n",
    "w^{(1)}_{31} & w^{(1)}_{32} & w^{(1)}_{33} \\\\\n",
    "w^{(1)}_{41} & w^{(1)}_{42} & w^{(1)}_{43} \\\\\n",
    "w^{(1)}_{51} & w^{(1)}_{51} & w^{(1)}_{53}\n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "x^{(0)}_0 \\\\ \n",
    "x^{(0)}_1 \\\\\n",
    "x^{(0)}_2 \\\\\n",
    "x^{(0)}_3 \\\\\n",
    "x^{(0)}_4 \\\\\n",
    "x^{(0)}_5 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w^{(1)}_{01} & w^{(1)}_{11} & w^{(1)}_{21} & w^{(1)}_{31} & w^{(1)}_{41} & w^{(1)}_{51} \\\\\n",
    "w^{(1)}_{02} & w^{(1)}_{12} & w^{(1)}_{22} & w^{(1)}_{32} & w^{(1)}_{42} & w^{(1)}_{52} \\\\\n",
    "w^{(1)}_{03} & w^{(1)}_{13} & w^{(1)}_{23} & w^{(1)}_{33} & w^{(1)}_{43} & w^{(1)}_{53} \\\\\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x^{(0)}_0 \\\\ \n",
    "x^{(0)}_1 \\\\\n",
    "x^{(0)}_2 \\\\\n",
    "x^{(0)}_3 \\\\\n",
    "x^{(0)}_4 \\\\\n",
    "x^{(0)}_5 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=0}^{5} w^{(1)}_{i1} x^{(0)}_i\\\\\n",
    "\\sum_{i=0}^{5} w^{(1)}_{i2} x^{(0)}_i\\\\\n",
    "\\sum_{i=0}^{5} w^{(1)}_{i3} x^{(0)}_i\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "s_1^{(1)}\\\\\n",
    "s_2^{(1)}\\\\\n",
    "s_3^{(1)}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The number of terms of the form $w^{(l)}_{ij} x^{(l-1)}_i$ is $3 \\cdot 6 = 18$ .\n",
    "\n",
    "Computing the output $\\mathbf{x}^{(1)}$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{(1)} \n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\theta(\\mathbf{s}^{(1)})\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "\\theta \\left( s_1^{(1)} \\right)\\\\\n",
    "\\theta \\left( s_2^{(1)} \\right)\\\\\n",
    "\\theta \\left( s_3^{(1)} \\right)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0 \\\\ \n",
    "x^{(1)}_1 \\\\\n",
    "x^{(1)}_2 \\\\\n",
    "x^{(1)}_3 \n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Note that layer $l=1$ contains a bias node.\n",
    "\n",
    "__________________________________________________________________\n",
    "\n",
    "- For $l = 2$:\n",
    "\n",
    "### Signal $\\mathbf{s}^{(2)}$ and output $\\mathbf{x}^{(2)}$ for layer 2\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{s}^{(2)}\n",
    "=\n",
    "(W^{(2)})^T \\mathbf{x}^{(1)}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w^{(2)}_{01}  \\\\\n",
    "w^{(2)}_{11}  \\\\\n",
    "w^{(2)}_{21}  \\\\\n",
    "w^{(2)}_{31} \n",
    "\\end{bmatrix}^T\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0 \\\\ \n",
    "x^{(1)}_1 \\\\\n",
    "x^{(1)}_2 \\\\\n",
    "x^{(1)}_3 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "w^{(2)}_{01}  & w^{(2)}_{11} & w^{(2)}_{21}  & w^{(2)}_{31}   \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0 \\\\ \n",
    "x^{(1)}_1 \\\\\n",
    "x^{(1)}_2 \\\\\n",
    "x^{(1)}_3 \n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\sum_{i=0}^{3} w^{(2)}_{i1} x^{(1)}_i\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "s_1^{(2)} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The number of terms of the form $w^{(l)}_{ij} x^{(l-1)}_i$ is $4 \\cdot 1 = 4$ .\n",
    "\n",
    "Computing the output $\\mathbf{x}^{(2)}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{x}^{(2)} \n",
    "=\n",
    "\\theta(\\mathbf{s}^{(2)})\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\theta \\left( s_1^{(2)} \\right)\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Note that the last layer $l = 2$ (output layer) has no bias node, see also e-Chapter 7, page 7.\n",
    "\n",
    "So in forward propagation we have $18 + 4 = 22$ operations of the form $w^{(l)}_{ij} x^{(l-1)}_i$. This is also equal to the number of weight edges in the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compute the sensitivities $\\boldsymbol{\\delta}^{(l)}$\n",
    "\n",
    "- Initialization: $\\delta^{(L)} = 2(x^{(L)} - y)(\\theta'(s^{(L)}))$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "In our case we have $L = 2$, so $\\delta^{(2)} = 2(x^{(2)} - y)(\\theta'(s^{(2)}))$.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "If we choose a transformation function $\\theta(s)$ whose derivative is $\\theta'(s) = (1-\\theta(s)^2)$, e.g. we could choose $\\theta(s) = \\tanh(s)$, see lecture 10, slide 18, then we get $\\theta'(s) = (1-\\theta(s)^2) = (1-x^2)$, and $\\delta^{(2)} = 2(x^{(2)} - y)(\\theta'(s^{(L)})) = 2(x^{(2)} - y)(1 - (x^{(2)})^2)$\n",
    "\n",
    "- Compute the sensitivity $\\boldsymbol{\\delta}^{(l)}$ from $\\boldsymbol{\\delta}^{(l-1)}$ via \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\boldsymbol{\\delta}^{(l)} = \\theta'(\\mathbf{s}^{(l)}) \\otimes \\left[ W^{(l+1)} \\boldsymbol{\\delta}^{(l+1)}  \\right]_1^d$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\otimes$ is the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_(matrices) between two matrices.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "If we choose a transformation function $\\theta(s)$ whose derivative is $\\theta'(s) = (1-\\theta(s)^2)$, e.g. we could choose $\\theta(s) = \\tanh(s)$, see lecture 10, slide 18, \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "then we have $\\theta'(\\mathbf{s}^{(l)}) = \\left[ 1 - \\theta(\\mathbf{s}^{(l)}) \\otimes \\theta(\\mathbf{s}^{(l)}) \\right] =  \\left[ 1 - \\mathbf{x}^{(l)} \\otimes \\mathbf{x}^{(l)} \\right]_1^{d^{(l)}}$. The latter notation with $1$ and $d^{(l)}$ means we take the components $1$ to $d^{(l)}$. \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "That is because the incoming signal $\\mathbf{s}^{(l)}$ has components $1$ to $d^{(l)}$  (each node $1$ to $d^{(l)}$ in layer $l$ has a signal), \n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "whereas the output $\\mathbf{x}^{(l)}$ has components $0$ to $d^{(l)}$ due to the bias node.\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "The computation then becomes\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\boldsymbol{\\delta}^{(l)} = \\left[ 1 - \\mathbf{x}^{(l)} \\otimes \\mathbf{x}^{(l)} \\right]_1^{d^{(l)}}  \\otimes \\left[ W^{(l+1)} \\boldsymbol{\\delta}^{(l+1)}  \\right]_1^{d^{(l)}}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "For $l = 1$ we get\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\boldsymbol{\\delta}^{(1)} = \\left[ 1 - \\mathbf{x}^{(1)} \\otimes \\mathbf{x}^{(1)} \\right]_1^{d^{(1)}}  \\otimes \\left[ W^{(2)} \\boldsymbol{\\delta}^{(2)}  \\right]_1^{d^{(1)}}$\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "$\\boldsymbol{\\delta}^{(1)} = \\left[ 1 - \\mathbf{x}^{(1)} \\otimes \\mathbf{x}^{(1)} \\right]_1^{3}  \\otimes \\left[ W^{(2)} \\boldsymbol{\\delta}^{(2)}  \\right]_1^{3}$\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\delta}^{(1)} \n",
    "= \n",
    "\\left[ \n",
    "1 \n",
    "-\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0 \\\\ \n",
    "x^{(1)}_1 \\\\\n",
    "x^{(1)}_2 \\\\\n",
    "x^{(1)}_3 \n",
    "\\end{bmatrix}\n",
    "\\otimes \n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0 \\\\ \n",
    "x^{(1)}_1 \\\\\n",
    "x^{(1)}_2 \\\\\n",
    "x^{(1)}_3 \n",
    "\\end{bmatrix} \n",
    "\\right]_1^{3}  \n",
    "\\otimes \n",
    "\\left[ \n",
    "\\begin{bmatrix}\n",
    "w^{(2)}_{01}  \\\\\n",
    "w^{(2)}_{11}  \\\\\n",
    "w^{(2)}_{21}  \\\\\n",
    "w^{(2)}_{31} \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\delta^{(2)}_1  \n",
    "\\end{bmatrix}\n",
    "\\right]_1^{3}\n",
    "\\end{equation}\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "Note that $\\boldsymbol{\\delta}^{(2)}$ is in general a vector. In our case it is a vector with one component, i.e. $\\boldsymbol{\\delta}^{(2)} = [\\delta^{(2)}_1]$\n",
    "\n",
    "\\begin{equation}\n",
    "\\boldsymbol{\\delta}^{(1)} \n",
    "= \n",
    "\\left[ \n",
    "1 \n",
    "-\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0 \\\\ \n",
    "x^{(1)}_1 \\\\\n",
    "x^{(1)}_2 \\\\\n",
    "x^{(1)}_3 \n",
    "\\end{bmatrix}\n",
    "\\otimes \n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0 \\\\ \n",
    "x^{(1)}_1 \\\\\n",
    "x^{(1)}_2 \\\\\n",
    "x^{(1)}_3 \n",
    "\\end{bmatrix} \n",
    "\\right]_1^{3}  \n",
    "\\otimes  \n",
    "\\begin{bmatrix}\n",
    "w^{(2)}_{01} \\delta^{(2)}_1 \\\\\n",
    "w^{(2)}_{11} \\delta^{(2)}_1 \\\\\n",
    "w^{(2)}_{21} \\delta^{(2)}_1 \\\\\n",
    "w^{(2)}_{31} \\delta^{(2)}_1\n",
    "\\end{bmatrix}_1^{3}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "\\delta^{(1)}_1 \\\\\n",
    "\\delta^{(1)}_2 \\\\\n",
    "\\delta^{(1)}_3 \n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
    "The latter vector contains 4 operations of the form $w_{ij}^{(l)} \\delta_j^{(l)}$ . If you take into account only the components from $1$ to $d^{(1)} = 3$, then there are only 3 operations of the form $w_{ij}^{(l)} \\delta_j^{(l)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Compute the partial derivatives $\\frac{\\partial e}{\\partial W^{(l)}}$\n",
    "\n",
    "In e-Chapter 7, page 12 we have equation (7.4) for the partial derivatives of the error $e$ with respect to every weight parameter $w_{ij}^{(l)}$ in the matrix $W^{(l)}$:\n",
    "\n",
    "$\\frac{\\partial e}{\\partial W^{(l)}} = \\mathbf{x}^{(l-1)} (\\boldsymbol{\\delta}^{(l)})^T$\n",
    "\n",
    "This is a matrix which contains the partial derivatives $\\frac{\\partial e}{\\partial w_{ij}^{(l)}}$ .\n",
    "\n",
    "These are required for the weight updates, see slide 20, lecture 10:\n",
    "\n",
    "$w_{ij}^{(l)} \\leftarrow w_{ij}^{(l)} - \\eta \\frac{\\partial e}{\\partial w_{ij}^{(l)}}$\n",
    "\n",
    "$w_{ij}^{(l)} \\leftarrow w_{ij}^{(l)} - \\eta x_i^{(l-1)} \\delta_j^{(l)} $\n",
    "\n",
    "Each weight parameter $w_{ij}^{(l)}$ is updated by substracting  the term $\\eta x_i^{(l-1)} \\delta_j^{(l)}$.\n",
    "\n",
    "So from this update expression we can already tell that the number of terms of the form $x_i^{(l-1)} \\delta_j^{(l)}$ is equal to the number of weight parameters $w_{ij}^{(l)}$. Let us nevertheless count the number of operations of the form $x_i^{(l-1)} \\delta_j^{(l)}$ explicitly.\n",
    "\n",
    "- for $l = 1$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial e}{\\partial W^{(1)}} = \\mathbf{x}^{(0)} (\\boldsymbol{\\delta}^{(1)})^T\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x^{(0)}_0 \\\\ \n",
    "x^{(0)}_1 \\\\\n",
    "x^{(0)}_2 \\\\\n",
    "x^{(0)}_3 \\\\\n",
    "x^{(0)}_4 \\\\\n",
    "x^{(0)}_5 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\delta^{(1)}_1 \\\\\n",
    "\\delta^{(1)}_2 \\\\\n",
    "\\delta^{(1)}_3 \n",
    "\\end{bmatrix}^T\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x^{(0)}_0 \\\\ \n",
    "x^{(0)}_1 \\\\\n",
    "x^{(0)}_2 \\\\\n",
    "x^{(0)}_3 \\\\\n",
    "x^{(0)}_4 \\\\\n",
    "x^{(0)}_5 \n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "\\delta^{(1)}_1 & \\delta^{(1)}_2 & \\delta^{(1)}_3\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x^{(0)}_0 \\delta^{(1)}_1 & x^{(0)}_0 \\delta^{(1)}_2 & x^{(0)}_0 \\delta^{(1)}_3 \\\\\n",
    "x^{(0)}_1 \\delta^{(1)}_1 & x^{(0)}_1 \\delta^{(1)}_2 & x^{(0)}_1 \\delta^{(1)}_3 \\\\\n",
    "x^{(0)}_2 \\delta^{(1)}_1 & x^{(0)}_2 \\delta^{(1)}_2 & x^{(0)}_2 \\delta^{(1)}_3 \\\\\n",
    "x^{(0)}_3 \\delta^{(1)}_1 & x^{(0)}_3 \\delta^{(1)}_2 & x^{(0)}_3 \\delta^{(1)}_3 \\\\\n",
    "x^{(0)}_4 \\delta^{(1)}_1 & x^{(0)}_4 \\delta^{(1)}_2 & x^{(0)}_4 \\delta^{(1)}_3 \\\\\n",
    "x^{(0)}_5 \\delta^{(1)}_1 & x^{(0)}_5 \\delta^{(1)}_2 & x^{(0)}_5 \\delta^{(1)}_3 \n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The number of operations of the form $x_i^{(l-1)} \\delta_j^{(l)}$ is $6 \\cdot 3 = 18$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for $l = 2$ we get:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial e}{\\partial W^{(2)}} = \\mathbf{x}^{(1)} (\\boldsymbol{\\delta}^{(2)})^T\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0 \\\\ \n",
    "x^{(1)}_1 \\\\\n",
    "x^{(1)}_2 \\\\\n",
    "x^{(1)}_3 \n",
    "\\end{bmatrix}\n",
    "[\\delta^{(2)}_1]\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "x^{(1)}_0 \\delta^{(2)}_1\\\\ \n",
    "x^{(1)}_1 \\delta^{(2)}_1\\\\\n",
    "x^{(1)}_2 \\delta^{(2)}_1\\\\\n",
    "x^{(1)}_3 \\delta^{(2)}_1\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "The number of operations of the form $x_i^{(l-1)} \\delta_j^{(l)}$ is $4 \\cdot 1 = 4$.\n",
    "\n",
    "Thus, using SGD on one data point involves $18 + 4 = 22$ operations of the form $x_i^{(l-1)} \\delta_j^{(l)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "The total number of operations from Step 0, Step 1 and Step 2 is $22 + 3 + 22 = 47$. Therefore, answer **8[d]** 45 is the correct answer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General case\n",
    "\n",
    "We can count the number of operations of the form $w^{(l)}_{ij} x^{(l-1)}_i$, $w_{ij}^{(l)} \\delta_j^{(l)}$ and $x_i^{(l-1)} \\delta_j^{(l)}$ if we are given the layer architecture $d = [d^{(0)}, d^{(1)}, ..., d^{(L)}]$.\n",
    "\n",
    "\n",
    "- We have seen in Step 0 (Foward propagation) that the number of operations $w^{(l)}_{ij} x^{(l-1)}_i$ equals the number of weight edges $w_{ij}^{(l)}$.\n",
    "- We have seen in Step 2 (Gradient) that the number of operations $w_{ij}^{(l)} \\delta_j^{(l)}$ is also equal to the number of weight edges $w_{ij}^{(l)}$. \n",
    "- The number of weight edges going into layer $l$ is equal to the matrix size $W^{(l)}$ with size $(d^{(l-1)}+1) \\times d^{(l)}$. Going through all layers we get: $\\sum_{l=1}^{L} \\text{size}(W^{(l)}) = \\sum_{l=1}^{L} ((d^{(l-1)}+1) \\times d^{(l)})$, see also e-Chapter 7 - page 9. So Step 0 and Step 2 together yield $2\\sum_{l=1}^{L} ((d^{(l-1)}+1) \\times d^{(l)})$ operations.\n",
    "- In Step 1 (Sensitivities $\\boldsymbol{\\delta}^{(l)}$) we calculated $\\boldsymbol{\\delta}^{(l)} = \\left[ 1 - \\mathbf{x}^{(l)} \\otimes \\mathbf{x}^{(l)} \\right]_1^{d^{(l)}}  \\otimes \\left[ W^{(l+1)} \\boldsymbol{\\delta}^{(l+1)}  \\right]_1^{d^{(l)}}$ for layers $l=1$ to $l=(L-1)$, see also e-Chapter 7 - page 14. The expression $\\left[ W^{(l+1)} \\boldsymbol{\\delta}^{(l+1)}  \\right]_1^{d^{(l)}}$ is a product of the two matrices $W^{(l+1)}$ and $\\boldsymbol{\\delta}^{(l+1)}$, where $W^{(l+1)}$ has size $(d^{(l)}+1) \\times d^{(l+1)}$ and $\\boldsymbol{\\delta}^{(l+1)} = \\frac{\\partial e}{\\partial \\mathbf{s}^{(l+1)}}$ has size $d^{(l+1)} \\times 1$ (see page 12 of e-Chapter 7). This means that the product $W^{(l+1)} \\boldsymbol{\\delta}^{(l+1)}$ has size $(d^{(l)}+1) \\times 1$. Finally, we can say that the matrix $[W^{(l+1)} \\boldsymbol{\\delta}^{(l+1)}]_1^{d^{(l)}}$ with entries $w_{ij}^{(l+1)} \\delta_j^{(l+1)}$ has size $d^{(l)} \\times 1$. If we go through layers $l=1$ to $l=(L-1)$, then the number of entries of the form $w_{ij}^{(l+1)} \\delta_j^{(l+1)}$ is $\\sum_{l=1}^{(L-1)} (d^{(l)} \\times 1) = \\sum_{l=1}^{(L-1)} d^{(l)}$. \n",
    "\n",
    "\n",
    "- We conclude that the total number of operations $N_{ops}$ of the form $w^{(l)}_{ij} x^{(l-1)}_i$, $w_{ij}^{(l)} \\delta_j^{(l)}$ and $x_i^{(l-1)} \\delta_j^{(l)}$ is $N_{ops} = 2\\sum_{l=1}^{L} ((d^{(l-1)}+1) \\times d^{(l)}) + \\sum_{l=1}^{(L-1)} d^{(l)}$.\n",
    "- Let's check our formula for our neural network. The architecture is $d = [d^{(0)}, d^{(1)}, d^{(2)}] = [5, 3, 1]$ with $L=2$. This yields $N_{ops} = 2\\sum_{l=1}^{L} ((d^{(l-1)}+1) \\times d^{(l)}) + \\sum_{l=1}^{(L-1)} d^{(l)} = 2 (6 \\cdot 3 + 4 \\cdot 1) + (3) = 47$.\n",
    "- For the the neural network in Example 7.1 on page 15 of e-Chapter 7 we have the architecture $d = [1, 2, 1, 1]$ with $L=3$. This yields $N_{ops} = 2\\sum_{l=1}^{L} ((d^{(l-1)}+1) \\times d^{(l)}) + \\sum_{l=1}^{(L-1)} d^{(l)} = 2 (2 \\cdot 2 + 3 \\cdot 1 + 2 \\cdot 1) + (2 + 1) = 21$  operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def N_ops(d):\n",
    "    '''\n",
    "    - Takes an architecture d.\n",
    "    - Returns the number of operations\n",
    "      of the form $w^{(l)}_{ij} x^{(l-1)}_i$, $w_{ij}^{(l)} \\delta_j^{(l)}$ and $x_i^{(l-1)} \\delta_j^{(l)}$\n",
    "    '''\n",
    "    total_ops = 0\n",
    "    L = len(d) - 1\n",
    "    \n",
    "    for i in range(1, L+1):\n",
    "        total_ops += ((d[i-1]+1) * d[i])\n",
    "    total_ops *= 2\n",
    "    \n",
    "    for i in range(1, (L-1)+1):\n",
    "        total_ops += d[i]\n",
    "        \n",
    "    return total_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of operations for d = [5, 3, 1] is N_ops = 47\n",
      "\n",
      "total number of operations for d = [1, 2, 1, 1] is N_ops = 21\n"
     ]
    }
   ],
   "source": [
    "d_homework = [5, 3, 1]\n",
    "print(\"total number of operations for d = {0} is N_ops = {1}\".format(d_homework, N_ops(d_homework)))\n",
    "\n",
    "d_example_7_1 = [1, 2, 1, 1]\n",
    "print(\"\\ntotal number of operations for d = {0} is N_ops = {1}\".format(d_example_7_1, N_ops(d_example_7_1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
