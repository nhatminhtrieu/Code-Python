{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 4\n",
    "\n",
    "## Problem 4\n",
    "\n",
    "For the model $h(x) = ax$ we have to compute $\\overline{g}(x) = \\hat{a} x$. \n",
    "\n",
    "Note that we calculate $\\overline{g}(x)$ via (see slide 6 of lecture 8) \n",
    "\n",
    "$\\overline{g}(x) \\approx \\sum_{i=1}^{K} g^{D_k}(x)$\n",
    "\n",
    "Since our model is $h(x) = ax$ this simplifies to:\n",
    "\n",
    "$\\overline{g}(x) \\approx \\frac{1}{K} \\sum_{i=1}^{K} g^{(D_i)}(x)$ \n",
    "\n",
    "$= \\frac{1}{K} \\sum_{i=1}^{K} a^{(D_i)} x$ \n",
    "\n",
    "$= x \\frac{1}{K} \\sum_{i=1}^{K} a^{(D_i)}$ \n",
    "\n",
    "$= x \\hat{a}$\n",
    "\n",
    "$= \\hat{a} x$\n",
    "\n",
    "which is why we can express $\\overline{g}(x)$ in terms of an average coefficient $\\hat{a}$.\n",
    "\n",
    "_____\n",
    "\n",
    "## Problem 5\n",
    "\n",
    "We have to compute the bias, see slide 9 of lecture 8:\n",
    "\n",
    "$bias = E_\\mathbf{x}[\\overline{g}(\\mathbf{x}) - f(\\mathbf{x})^2]$\n",
    "\n",
    "The bias is the distance of $\\overline{g}(\\mathbf{x})$ to $f(\\mathbf{x})$\n",
    "\n",
    "**Method 1:**\n",
    "\n",
    "We can do this by choosing $N_{test} = 1000$ test points and by calculating the average square difference $(\\overline{g}(\\mathbf{x}) - f(\\mathbf{x})^2)$\n",
    "\n",
    "$bias \\approx \\frac{1}{N_{test}} \\sum_{i=1}^{N_{test}} [\\overline{g}(\\mathbf{x}_i) - f(\\mathbf{x}_i)^2]$\n",
    "\n",
    "**Method 2:**\n",
    "\n",
    "We can use an integral to compute $E_\\mathbf{x}[\\overline{g}(\\mathbf{x}) - f(\\mathbf{x})^2]$ .\n",
    "\n",
    "This is equivalent to calculating the mean of a function $r(x)$ in the interval $[a,b]$ via\n",
    "\n",
    "$\\bar {r}(x) = \\tfrac 1 {b-a} \\int_{a}^b r(x) dx$\n",
    "\n",
    "see also [here](https://www.math.vt.edu/people/qlfang/class_home/Lesson8.pdf) and [here](https://en.wikipedia.org/wiki/Mean_of_a_function) .\n",
    "\n",
    "With $r(x) = (\\bar g(x) - f(x))^2$ we get:\n",
    "\n",
    "$\\bar {r}(x) = \\tfrac 1 {b-a} \\int_{a}^b (\\bar g(x) - f(x))^2 dx =: bias$\n",
    "____\n",
    "\n",
    "## Problem 6\n",
    "\n",
    "We have to compute the variance, see slide 9 of lecture 8:\n",
    "\n",
    "$E_\\mathbf{x}[E_D[g^{(D)}(x) - \\bar{g}(x) ]]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "solution problem 4: a_avg =  1.40845852709\n",
      "Answer 4[e] is therefore correct.\n",
      "\n",
      "Solution to problem 5: bias =  0.279449588863\n",
      "Answer 5[b] is therefore correct.\n",
      "\n",
      "Solution to problem 6, variance =  0.239356400027\n",
      "Answer 6[a] is therefore correct.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def problem4():\n",
    "    \n",
    "    RUNS = 1000\n",
    "    a_total = 0\n",
    "    N = 2          # size of data set\n",
    "    \n",
    "    for _ in range(RUNS):\n",
    "        # two random points\n",
    "        x_rnd = np.random.uniform(-1, 1, N)      # this is a vector of size 2\n",
    "        y_rnd = np.sin(np.pi * x_rnd)            # this is a vector of size 2\n",
    "\n",
    "        # linear regression for model y = ax\n",
    "        X_a = np.array([x_rnd]).T\n",
    "        w_a = np.dot(np.dot(np.linalg.inv(np.dot(X_a.T, X_a)), X_a.T), y_rnd)\n",
    "        a = w_a[0]\n",
    "\n",
    "        a_total += a\n",
    "        \n",
    "    a_avg = a_total / RUNS\n",
    "    return a_avg\n",
    "\n",
    "\n",
    "print(\"solution problem 4: a_avg = \", problem4())\n",
    "print(\"Answer 4[e] is therefore correct.\")\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------\n",
    "\n",
    "def problem5():\n",
    "    N_test = 1000\n",
    "    x_test = np.random.uniform(-1,1,N_test)\n",
    "\n",
    "    y_f = np.sin(np.pi * x_test)\n",
    "    a_avg = problem4()\n",
    "    y_g_bar = a_avg * x_test\n",
    "\n",
    "    bias = sum((y_f - y_g_bar)**2) / N_test\n",
    "    return bias\n",
    "    \n",
    "    \n",
    "print(\"\\nSolution to problem 5: bias = \", problem5())\n",
    "print(\"Answer 5[b] is therefore correct.\")\n",
    "\n",
    "#--------------------------------------------------------------------------\n",
    "\n",
    "def problem6():\n",
    "    a_avg = problem4()\n",
    "    expectation_over_X = 0\n",
    "    \n",
    "    RUNS_D = 100\n",
    "    RUNS_X = 1000\n",
    "    # variance: Compare each g to g_bar\n",
    "    \n",
    "    for i in range(RUNS_X):\n",
    "        N = 2\n",
    "        x_test = np.random.uniform(-1,1)\n",
    "        expectation_over_D = 0\n",
    "        \n",
    "        for _ in range(RUNS_D):\n",
    "            # two random points as data set D\n",
    "            x_rnd = np.random.uniform(-1, 1, N)\n",
    "            y_rnd = np.sin(np.pi * x_rnd)\n",
    "\n",
    "            # linear regression for model y = ax\n",
    "            # get a particular g^(D)\n",
    "            X_a = np.array([x_rnd]).T\n",
    "            w_a = np.dot(np.dot(np.linalg.inv(np.dot(X_a.T, X_a)), X_a.T), y_rnd)\n",
    "            a  = w_a[0]\n",
    "            \n",
    "            # calculate difference\n",
    "            y_g = a * x_test\n",
    "            y_g_bar = a_avg * x_test\n",
    "            expectation_over_D += (y_g - y_g_bar)**2 / RUNS_D\n",
    "\n",
    "        expectation_over_X += expectation_over_D / RUNS_X\n",
    "    \n",
    "    variance = expectation_over_X\n",
    "    return variance\n",
    "\n",
    "\n",
    "print(\"\\nSolution to problem 6, variance = \", problem6())\n",
    "print(\"Answer 6[a] is therefore correct.\")\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the **bias via the integral**:\n",
    "\n",
    "\n",
    "$bias = \\tfrac 1 {1-(-1)} \\int_{-1}^{1} (\\hat{a}x - \\sin(\\pi x))^2 dx\n",
    "= \\tfrac 1 {2} \\int_{-1}^{1} (1.43 x - \\sin(\\pi x))^2 $\n",
    "\n",
    "which in WolframAlpha evaluated equals 0.27\n",
    "\n",
    "https://www.wolframalpha.com/input/?i=integral+from+-1+to+1+(1.43+x+-+sin(pi+*+x))%5E2+%2F+2\n",
    "\n",
    "This coincides with our Monte Carlo simulation with $N_{test} = 1000$ points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing program by confirming values from lecture\n",
    "\n",
    "Below we test our program by confirming the values from slide 15, lecture 8 with\n",
    "\n",
    "- $bias = 0.21$\n",
    "- $var = 1.69$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "bias lecture:  0.210732294044\n",
      "variance =  1.68353665926\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def get_g_bar_lecture():\n",
    "\n",
    "    RUNS = 1000\n",
    "    m_total = 0\n",
    "    b_total = 0\n",
    "\n",
    "    x = np.arange(-1, 1.1, .1)\n",
    "    y = np.sin(np.pi * x)\n",
    "\n",
    "    N = 2\n",
    "\n",
    "    for _ in range(RUNS):\n",
    "\n",
    "        # two random points\n",
    "        x_rnd = np.random.uniform(-1, 1, N)\n",
    "        y_rnd = np.sin(np.pi * x_rnd)\n",
    "\n",
    "        # linear regression for model y = mx + b\n",
    "        X = np.array([np.ones(N), x_rnd]).T\n",
    "        w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y_rnd)\n",
    "\n",
    "        b, m = w\n",
    "        x_line = np.array([-1, 1])\n",
    "        y_line = m * x_line + b\n",
    "\n",
    "        m_total += m\n",
    "        b_total += b\n",
    "\n",
    "    # https://matplotlib.org/examples/pylab_examples/axes_props.html\n",
    "\n",
    "    m_avg = m_total/RUNS\n",
    "    b_avg = b_total/RUNS\n",
    "\n",
    "    '''\n",
    "    plt.plot(x, y)\n",
    "    plt.plot(x_rnd, y_rnd, 'go')\n",
    "    plt.plot(x_line, y_line, 'r-')\n",
    "    plt.plot(x_line_a, y_line_a, 'b-')\n",
    "    plt.ylim(-1,1)\n",
    "    plt.xlim(-1,1)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "    '''\n",
    "\n",
    "\n",
    "    # Calculate bias and variance\n",
    "\n",
    "    # bias: compare g_bar(x) to f(x)\n",
    "    # generate 1000 random points\n",
    "    N_test = 1000\n",
    "    x_test = np.random.uniform(-1,1,N_test)\n",
    "\n",
    "    y_f = np.sin(np.pi * x_test)\n",
    "    y_g_bar_lecture = m_avg * x_test + b_avg\n",
    "\n",
    "    bias_lecture = sum((y_f - y_g_bar_lecture)**2) / N_test\n",
    "    print(\"\\n\\nbias lecture: \", bias_lecture)\n",
    "\n",
    "    return (m_avg, b_avg)\n",
    "    \n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------\n",
    "\n",
    "def confirm_variance_from_lecture():\n",
    "    m_avg, b_avg = get_g_bar_lecture()\n",
    "    expectation_over_X = 0\n",
    "    \n",
    "    RUNS_D = 100\n",
    "    RUNS_X = 1000\n",
    "    # variance: Compare each g to g_bar\n",
    "    \n",
    "    for i in range(RUNS_X):\n",
    "        N = 2\n",
    "        x_test = np.random.uniform(-1,1)\n",
    "        expectation_over_D = 0\n",
    "        for _ in range(RUNS_D):\n",
    "            # two random points as data set D\n",
    "            x_rnd = np.random.uniform(-1, 1, N)\n",
    "            y_rnd = np.sin(np.pi * x_rnd)\n",
    "\n",
    "            # linear regression for model y = mx + b\n",
    "            X = np.array([np.ones(N), x_rnd]).T\n",
    "            w = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y_rnd)\n",
    "            b, m = w\n",
    "\n",
    "            # calculate difference\n",
    "            y_g = m * x_test + b\n",
    "            y_g_bar = m_avg * x_test + b_avg\n",
    "            expectation_over_D += (y_g - y_g_bar)**2 / RUNS_D\n",
    "        #print(\"expectation_over_D = \", expectation_over_D)\n",
    "        expectation_over_X += expectation_over_D / RUNS_X\n",
    "    \n",
    "    print(\"variance = \", expectation_over_X)\n",
    "\n",
    "        \n",
    "confirm_variance_from_lecture()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
