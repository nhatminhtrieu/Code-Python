{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOWfRgoZDrsn"
   },
   "source": [
    "# Lab: Neural Net\n",
    "\n",
    "(Last update: 16/04/2022)\n",
    "\n",
    "- Student ID: \n",
    "- Student name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8eazZSw1Drsq"
   },
   "source": [
    "---\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicates the part you need to do.  You can discuss ideas with classmates as well as find information from the internet, book, ... but *this lab must be your*.\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`). Then create a folder named `ID` (for example, if your ID is 1234567, then name the folder `1234567`). Copy the notebook file (*do not copy the data file*) to this folder, compress and submit it on moodle.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What problem do we want to solve in this lab?**\n",
    "\n",
    "Given the training data: \n",
    "$$\\{(\\textbf{x}^{(1)}, y^{(1)}), ..., (\\textbf{x}^{(N)}, y^{(N)})\\}$$\n",
    "where:\n",
    "\n",
    "- $\\textbf{x}^{(n)} \\in \\mathbb{R}^{784}$ is an input vector containing pixel values of a $28 \\times 28$ grayscale image of some hand-written digit\n",
    "- $y^{(n)} \\in \\{0, 1, 2, ..., 9\\}$ is the corresponding output indicating which digit\n",
    "\n",
    "Our task is building a model (in this lab: a Fully-Connected Feed-Forward Neural Net) from this data so that it can take a *new* hand-written digit image (a vector $\\in \\mathbb{R}^{784}$) as input and predict the output (which digit) *well*.\n",
    "\n",
    "\n",
    "Are you ready? Let's start!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will use Numpy as main lib in this lab\n",
    "# So, you should use Numpy operations on Numpy arrays, avoid using loops;\n",
    "# otherwise, the code will run slow\n",
    "import numpy as np \n",
    "\n",
    "# Import other libs\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# We will not use anything else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data and explore a little bit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specific data we will use in this lab is the famous MNIST dataset of hand-written digit images. [The original MNIST dataset](http://yann.lecun.com/exdb/mnist/) contains 2 sets: training set (60000 images) and test set (10000 images). The MNIST dataset we will use in this lab (the \"mnist.pkl.gz\" file) contains 3 sets: training set (50000 images), validation set (10000 images), and test set (10000 images); the training set and the validation set here were created by splitting the original training set (60000 images).  \n",
    "\n",
    "The code below will read data from the \"mnist.pkl.gz\" file (you shold put the \"mnist.pkl.gz\" file to the same folder with this notebook file) and store data in 6 Numpy arrays:\n",
    "\n",
    "- `train_X`, `train_Y`\n",
    "- `val_X`, `val_Y`\n",
    "- `test_X`, `test_Y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mnist(mnist_file):\n",
    "    '''\n",
    "    Reads MNIST data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mnist_file : string\n",
    "        The name of the MNIST file (e.g., 'mnist.pkl.gz').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (train_X, train_Y, val_X, val_Y, test_X, test_Y) : tuple\n",
    "        train_X : numpy array, shape (N=50000, d=784)\n",
    "            Input vectors of the training set.\n",
    "        train_Y : numpy array, shape (N=50000)\n",
    "            Outputs of the training set.\n",
    "        val_X : numpy array, shape (N=10000, d=784)\n",
    "            Input vectors of the validation set.\n",
    "        val_Y : numpy array, shape (N=10000)\n",
    "            Outputs of the validation set.\n",
    "        test_X : numpy array, shape (N=10000, d=784)\n",
    "            Input vectors of the test set.\n",
    "        test_Y : numpy array, shape (N=10000)\n",
    "            Outputs of the test set.\n",
    "    '''\n",
    "    \n",
    "    f = gzip.open(mnist_file, 'rb')\n",
    "    train_data, val_data, test_data = pickle.load(f, encoding='latin1')\n",
    "    f.close()\n",
    "    \n",
    "    train_X, train_Y = train_data\n",
    "    val_X, val_Y = val_data\n",
    "    test_X, test_Y = test_data    \n",
    "    \n",
    "    return train_X, train_Y, val_X, val_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_Y, val_X, val_Y, test_X, test_Y = read_mnist('mnist.pkl.gz')\n",
    "\n",
    "print('train_X.shape =', train_X.shape)\n",
    "print('train_Y.shape =', train_Y.shape)\n",
    "print('val_X.shape   =', val_X.shape)\n",
    "print('val_Y.shape   =', val_Y.shape)\n",
    "print('test_X.shape  =', test_X.shape)\n",
    "print('test_Y.shape  =', test_Y.shape)\n",
    "\n",
    "print('\\ntrain_X: min = %.3f, max = %.3f' %(train_X.min(), train_X.max()))\n",
    "print('train_Y: min = %d, max = %d' %(train_Y.min(), train_Y.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsDjR6u6Drst"
   },
   "source": [
    "## Build a good Neural Net from data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement functions to train a Neural Net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use a simple form of Neural Net: Fully-Connected Feed-Forward Neural Net. We will use sigmoid function as activation function in hidden layers *as well as in output layer* (using sigmoid instead of softmax in output layer will make our life easier). We will have $K$ neurons in output layer with $K$ is the number of classes (in our case, $K = 10$); the output of each neuron will indicate the probability a given input vector belonging to the corresponding class (note that sum of all these probabilities will not equal 1 as softmax). We can choose the class with highest probability as the predicted class for a given input vector.\n",
    "\n",
    "\n",
    "In our first step, we will implement `compute_nnet_outputs` function *(3 points)*. We will use this function not only after training (to compute outputs of our trained Neural Net with new input vectors) but also during the training process (which we will tackle in our next step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(S):\n",
    "    '''\n",
    "    Computes sigmoid function for each element of array S.\n",
    "    You can use this function in `compute_nnet_outputs` function.\n",
    "    '''\n",
    "    return 1 / (1 + np.exp(-S))\n",
    "\n",
    "def compute_nnet_outputs(Ws, X, need_all_layer_outputs):\n",
    "    '''\n",
    "    Computes the outputs of Neural Net by forward propagating X through the net.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Ws : list of numpy arrays\n",
    "        Ws[l-1] is W of layer l with l >= 1 (layer 0 is input layer, \n",
    "        it doesn't have W); W of layer l will have the shape of \n",
    "        (d^(l-1)+1, d^(l)), where  d^(l-1) is the number of neurons \n",
    "        (not count the +1 neuron) of layer l-1 and  d^(l) is the number of \n",
    "        neurons (not count the +1 neuron) of layer l.\n",
    "    X : numpy array, shape (N, d+1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    need_all_layer_outputs : bool\n",
    "        If this var is true, we'll return a list of layer's-outputs (we'll \n",
    "        need this list when training); otherwise, we'll return the final \n",
    "        layer's output.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    If `need_all_layer_outputs` is false, return\n",
    "        A : numpy array, shape (N, K=10)\n",
    "            The maxtrix of output vectors of final layer; each row is an \n",
    "            output vector (containing each class's probability given the \n",
    "            corresponding input vector).\n",
    "    Else, return\n",
    "        As : list of numpy arrays\n",
    "            As[l] is the matrix of output vectors of layer l (l=0 will \n",
    "            correspond to input layer); each row is an output vector \n",
    "            (corresponding to an input vector); if layer l is not the final \n",
    "            layer, the first column of this matrix is all ones.\n",
    "    '''    \n",
    "    \n",
    "    # TODO\n",
    "    # NOTE: to make the code run fast, you should use Numpy operations on\n",
    "    # Numpy arrays; your code should have only one loop to loop through layers\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK THE CORRECTNESS OF YOUR COMPUTE_NNET_OUTPUTS FUNCTION\n",
    "\n",
    "# A small X with 4 rows corresponding to 4 input vectors\n",
    "X = np.array([[1.0, 0.9, 0.9], \n",
    "              [1.0, 0.5, 0.4], \n",
    "              [1.0, 0.4, 0.5],\n",
    "              [1.0, 0.1, 0.7]])\n",
    "# A small neural net: \n",
    "# 2 input neurons - 3 hidden neurons - 2 hidden neurons - 1 output neurons\n",
    "# (not counting +1 neurons)\n",
    "Ws = [np.array([[-0.3 ,  0.2 ,  0.5 ],\n",
    "                [-0.1 , -0.2 , -0.35],\n",
    "                [ 0.45, -0.7 , -0.7 ]]),\n",
    "      np.array([[ 0.3 , -0.05],\n",
    "                [ 0.6 ,  0.3 ],\n",
    "                [-0.8 , -0.3 ],\n",
    "                [ 0.4 , -0.45]]),\n",
    "      np.array([[-0.3 ],\n",
    "                [ 0.5 ],\n",
    "                [-0.45]])]\n",
    "\n",
    "# Check your compute_nnet_outputs function!\n",
    "A = compute_nnet_outputs(Ws, X, False)\n",
    "assert np.array_equal(np.round(A, 5),\n",
    "                      np.array([[0.45109],\n",
    "                                [0.45199],\n",
    "                                [0.4521 ],\n",
    "                                [0.45247]]))\n",
    "As = compute_nnet_outputs(Ws, X, True)\n",
    "assert len(As) == 4\n",
    "assert np.array_equal(np.round(As[0], 5),\n",
    "                      np.array([[1. , 0.9, 0.9],\n",
    "                                [1. , 0.5, 0.4],\n",
    "                                [1. , 0.4, 0.5],\n",
    "                                [1. , 0.1, 0.7]]))\n",
    "assert np.array_equal(np.round(As[1], 5),\n",
    "                      np.array([[1.     , 0.50375, 0.35206, 0.39055],\n",
    "                                [1.     , 0.4576 , 0.45512, 0.51125],\n",
    "                                [1.     , 0.47128, 0.44275, 0.5025 ],\n",
    "                                [1.     , 0.50125, 0.42311, 0.49375]]))\n",
    "assert np.array_equal(np.round(As[2], 5),\n",
    "                      np.array([[1.     , 0.617  , 0.45506],\n",
    "                                [1.     , 0.60228, 0.43062],\n",
    "                                [1.     , 0.60577, 0.4335 ],\n",
    "                                [1.     , 0.61296, 0.43813]]))\n",
    "assert np.array_equal(np.round(As[3], 5),\n",
    "                      np.array([[0.45109],\n",
    "                                [0.45199],\n",
    "                                [0.4521 ],\n",
    "                                [0.45247]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will implement our main function: `train_nnet` *(4 points)*. Basically, in this function, we will find values of our Neural Net weights by minimizing mean cross-entropy error on the training set  $\\{(\\textbf{x}^{(1)}, y^{(1)}), ..., (\\textbf{x}^{(N)}, y^{(N)})\\}$:\n",
    "$$E(w\\text{'s of our Net}) = \\frac{1}{N} \\sum_{n=1}^Ne(h(\\textbf{x}^{(n)}), \\textbf{y}^{(n)}))$$\n",
    "where:\n",
    "- $h(\\textbf{x}^{(n)}) \\in \\mathbb{R}^{K}$ is the output vector (containing probabilities) of our Neural Net corresponding to the input vector $\\textbf{x}^{(n)}$\n",
    "- $\\textbf{y}^{(n)} \\in \\mathbb{R}^{K}$ is the one-hot representation of $y^{(n)}$ (can you see the difference between  $\\textbf{y}^{(n)}$ and $y^{(n)}$?) \\\n",
    "For example, in our case $K=10$ (10 digits):\n",
    "    - If $y^{(n)}=0$ (digit 0) then $\\textbf{y}^{(n)}=[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]^T$\n",
    "    - If $y^{(n)}=1$ (digit 1) then $\\textbf{y}^{(n)}=[0, 1, 0, 0, 0, 0, 0, 0, 0, 0]^T$\n",
    "- $e(h(\\textbf{x}^{(n)}), \\textbf{y}^{(n)}))$ is the cross-entropy error on a single training pair $(\\textbf{x}^{(n)}, y^{(n)})$:\n",
    "$$e(h(\\textbf{x}^{(n)}), \\textbf{y}^{(n)})) = \\sum_{k=1}^K - y^{(n)}_k \\ln h(\\textbf{x}^{(n)})_k - (1 - y^{(n)}_k)\\ln (1 - h(\\textbf{x}^{(n)})_k)$$\n",
    "\n",
    "If we use GD (Gradient Descent) to minimize $E(w\\text{'s of our Net})$, we will need to compute partial derivatives of $E$ with respect to $w\\text{'s}$. To compute these partial derivatives, we will compute partial derivatives of $e$ (cross-entropy error on a single training pair $\\mathbf{x}$ and $y$) with respect to $w\\text{'s}$, and then take average over all training pairs. But it will slow when our training set is big. SGD (Stochastic Gradient Descent) approximates the average of partial derivatives over all training pairs by the average of partial derivatives over a mini-batch (a subset of training pairs); so, it's faster than GD. Another name of SGD: \"túy quyền\" ;-). In `train_nnet` function below, we will use SGD.\n",
    "\n",
    "Let's focus on how to compute the partial derivative of $e$ (cross-entropy error on a single training pair $\\mathbf{x}$ and $y$) with respect to a weight $w$ between a neuron i in layer l-1 and a neuron j in layer l. After applying chain rule, we have the formula: \n",
    "$$\\text{this partial derivative} = \\text{output of neuron i in layer l-1} \\times \\text{delta of neuron j in layer l}$$\n",
    "with delta of a neuron is the partial derivative of $e$ with respect to the weighted sum (value before applying activation function) of that neuron.\n",
    "\n",
    "I have implemented most of `train_nnet` function for you. Your job is to write 3 lines of code corresponding to 3/4 tasks below:\n",
    "\n",
    "**Task 1. Compute delta's of the last layer (on a mini-batch)**\n",
    "\n",
    "For example, let's consider $K=3$ (3 classes) and a mini-batch with size of 2 (2 training pairs), if:\n",
    "\n",
    "        A = np.array([[0.8, 0.7, 0.6],\n",
    "                      [0.5, 0.6, 0.5]]) # Output vectors of last layer \n",
    "                                          on this mini-batch\n",
    "        mb_Y = np.array([[0, 1, 0],\n",
    "                         [1, 0, 0]]) # Correct one-hot output vectors \n",
    "                                       of this mini-batch\n",
    "then delta's of the last layer on this mini-batch will be:\n",
    "\n",
    "        delta = np.array([[ 0.8, -0.3,  0.6],\n",
    "                          [-0.5,  0.6,  0.5]])\n",
    "You should do the math to figure out the exact formula to compute delta's of the last layer. In case it's difficult for you, try your luck and guess the formula from the result above ;-). Again, when implementing this task, you should use Numpy operations on Numpy arrays; this task should be done with just one line of code.\n",
    "\n",
    "**Task 2. Compute gradient - a collection of partial derivatives - of the last layer from delta's of the last layer and outputs of the previous layer (on a mini-batch)**\n",
    "\n",
    "For example, let's consider $K=3$ (3 classes; it's also the number of neurons in the last layer L), the previous layer (layer L-1) with 4 neurons, and a mini-batch with size of 2 (2 training pairs), if:\n",
    "\n",
    "        delta = np.array([[ 0.8, -0.3,  0.6],\n",
    "                          [-0.5,  0.6,  0.5]]) # Delta vectors of last layer \n",
    "                                                 on this mini-batch\n",
    "        A = np.array([[1.0, 0.5, 0.1, 0.3, 0.2],\n",
    "                      [1.0, 0.9, 0.8, 0.7, 0.1]]) # Output vectors of the previous layer \n",
    "                                                    on this mini-batch\n",
    "then the gradient of the last layer will be (it has the same shape with `W` array of the last layer):\n",
    "\n",
    "        grad = np.array([[ 0.15 ,  0.15 ,  0.55 ],\n",
    "                         [-0.025,  0.195,  0.375],\n",
    "                         [-0.16 ,  0.225,  0.23 ],\n",
    "                         [-0.055,  0.165,  0.265],\n",
    "                         [ 0.055,  0.   ,  0.085]]) # For clarity, I use \n",
    "                                                      np.round(..., 3) to show this\n",
    "How is `grad` computed from `delta` and `A`? Hint: I have mentioned above how to compute the partial derivative of cross-entropy error on a single training pair with respect to a weight; for a mini-batch, just compute it for each training pair and then take average.\n",
    "\n",
    "Again, when implementing this task, you should use Numpy operations on Numpy arrays; this task should be done with just one line of code.\n",
    "\n",
    "**Task 3. Compute delta's of layer l from delta's of layer l+1 (on a mini-batch)**\n",
    "\n",
    "(Note that we will not compute delta's of +1 neurons)\n",
    "\n",
    "This is the most difficult task, but I have done it for you :-). \n",
    "\n",
    "\n",
    "**Task 4. Compute gradient of layer l from delta of layer l and outputs of layer l-1 (on a mini-batch)**\n",
    "\n",
    "It's similar to task 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_nnet(X, Y, val_X, val_Y, \n",
    "               hid_layer_sizes, \n",
    "               mb_size, learning_rate, max_epoch):\n",
    "    '''\n",
    "    Trains Neural Net on the dataset (X, Y); also prints out mean binary error \n",
    "    (the percentage of misclassified data points) on training set and \n",
    "    validation set during training\n",
    "    Cost function: mean cross-entropy error\n",
    "    Optimization algorithm: SGD\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (N, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    Y : numpy array, shape (N,) \n",
    "        The vector of outputs.\n",
    "    val_X, val_Y : validation data, similar to X and Y\n",
    "    hid_layer_sizes : list\n",
    "        The list of hidden layer sizes; e.g., hid_layer_sizes = [20, 10] means\n",
    "        the Net has 2 hidden layers, the 1st one has 20 neurons, and the 2nd \n",
    "        one has 10 neurons (not count the +1 neurons).\n",
    "    mb_size : int\n",
    "        Minibatch size of SGD.\n",
    "    learning_rate : float\n",
    "        Learning rate of SGD.\n",
    "    max_epoch : int\n",
    "        After this number of epochs, we'll terminate SGD.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    (Ws, costs, errs) : tuple\n",
    "        Ws : list of numpy arrays\n",
    "            Ws[l-1] is W of layer l with l >= 1 (layer 0 is input layer, \n",
    "            it doesn't have W); W of layer l will have the shape of \n",
    "            (d^(l-1)+1, d^(l)), where d^(l-1) is the number of neurons \n",
    "            (not count the +1 neuron) of layer l-1 and d^(l) is the number of \n",
    "            neurons (not count the +1 neuron) of layer l.\n",
    "        costs : list, len = max_epoch\n",
    "            The list of costs after each epoch.\n",
    "        errs : list, len = max_epoch\n",
    "            The list of mean binary errors (on the training set) after each epoch.\n",
    "    '''\n",
    "    \n",
    "    # Prepare for training\n",
    "    K = len(np.unique(Y)) # Num classes\n",
    "    layer_sizes = [X.shape[1] - 1] + hid_layer_sizes + [K]\n",
    "    np.random.seed(0) # This will fix the randomization; \n",
    "                      # so, you and me will have the same results\n",
    "    Ws = [np.random.randn(layer_sizes[i] + 1, layer_sizes[i + 1]) \n",
    "          / np.sqrt(layer_sizes[i] + 1) \n",
    "          for i in range(len(layer_sizes) - 1)] # Secret formula to init Ws ;-)\n",
    "    one_hot_Y = np.zeros((len(Y), K))\n",
    "    one_hot_Y[np.arange(len(Y)), Y] = 1\n",
    "    errs = [] # To save mean binary errors on training set during training\n",
    "    val_errs = [] # To save mean binary errors on validation set during training\n",
    "    N = len(X) # Num training examples\n",
    "    rnd_idxs = np.arange(N) # Random indexes    \n",
    "    \n",
    "    # Train\n",
    "    for epoch in range(max_epoch):\n",
    "        np.random.shuffle(rnd_idxs)\n",
    "        for start_idx in range(0, N, mb_size):\n",
    "            # Get minibach\n",
    "            mb_X = X[rnd_idxs[start_idx:start_idx+mb_size]]\n",
    "            mb_Y = one_hot_Y[rnd_idxs[start_idx:start_idx+mb_size]]\n",
    "            \n",
    "            # Forward-prop\n",
    "            As = compute_nnet_outputs(Ws, mb_X, True)\n",
    "            \n",
    "            # Back-prop; on the way, compute each layer's gradient and update its W\n",
    "            # TODO: delta = ... (task 1)\n",
    "            \n",
    "            # TODO: grad = ... (task 2)\n",
    "            \n",
    "            Ws[-1] -= learning_rate * grad\n",
    "            for i in range(2, len(Ws) + 1):\n",
    "                delta = delta.dot(Ws[-i + 1].T[:, 1:]) * As[-i][:, 1:] * (1 - As[-i][:, 1:])\n",
    "                # TODO: grad = ... (task 4)\n",
    "                \n",
    "                Ws[-i] -= learning_rate * grad\n",
    "        \n",
    "        # Compute training info, save it, and print it\n",
    "        A = compute_nnet_outputs(Ws, X, False)\n",
    "        err = np.mean(np.argmax(A, axis=1) != Y) * 100\n",
    "        errs.append(err)\n",
    "        val_A = compute_nnet_outputs(Ws, val_X, False)\n",
    "        val_err = np.mean(np.argmax(val_A, axis=1) != val_Y) * 100\n",
    "        val_errs.append(val_err)\n",
    "        print('Epoch %d, train err %.3f%%, val err %.3f%%' %(epoch, err, val_err))\n",
    "            \n",
    "    return Ws, errs, val_errs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK THE CORRECTNESS YOUR TRAIN_NNET FUCTION\n",
    "Ws, train_errs, val_errs = train_nnet(train_X, train_Y, val_X, val_Y,\n",
    "                                      hid_layer_sizes=[20], \n",
    "                                      mb_size=32, learning_rate=0.1, max_epoch=3)\n",
    "assert np.array_equal(np.round(train_errs, 3),\n",
    "                      np.array([10.982, 9.044, 7.962]))\n",
    "assert np.array_equal(np.round(val_errs, 3),\n",
    "                      np.array([9.82, 8.23, 7.49]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use implemented functions to train different Neural Nets and choose the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shallow Neural Net\n",
    "Ws_50, train_errs_50, val_errs_50 = \\\n",
    "    train_nnet(train_X, train_Y, val_X, val_Y, \n",
    "               hid_layer_sizes=[50], \n",
    "               mb_size=32, learning_rate=0.1, max_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deeper Neural Net\n",
    "Ws_50_50, train_errs_50_50, val_errs_50_50 = \\\n",
    "    train_nnet(train_X, train_Y, val_X, val_Y, \n",
    "               hid_layer_sizes=[50, 50], \n",
    "               mb_size=32, learning_rate=0.1, max_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Even more deeper Neural Net!\n",
    "Ws_50_50_50, train_errs_50_50_50, val_errs_50_50_50 = \\\n",
    "    train_nnet(train_X, train_Y, val_X, val_Y, \n",
    "               hid_layer_sizes=[50, 50, 50], \n",
    "               mb_size=32, learning_rate=0.1, max_epoch=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.log(train_errs_50), color='blue', \n",
    "         label='Net 50 - train err')\n",
    "plt.plot(np.log(val_errs_50), color='blue', linestyle='--', \n",
    "         label='Net 50 - val err')\n",
    "plt.plot(np.log(train_errs_50_50), color='green',\n",
    "         label='Net 50_50 - train err')\n",
    "plt.plot(np.log(val_errs_50_50), color='green', linestyle='--',\n",
    "         label='Net 50_50 - val err')\n",
    "plt.plot(np.log(train_errs_50_50_50), color='red',\n",
    "         label='Net 50_50_50 - train err')\n",
    "plt.plot(np.log(val_errs_50_50_50), color='red', linestyle='--',\n",
    "         label='Net 50_50_50 - val err')\n",
    "plt.legend()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Ln of error (for clarity)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: comment on the graph above (Why is it like this? In case you don't know why, just simply say so) *(2 points)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose the Neural Net model having smallest mean binary error on the validation set as our best Neural Net model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How good our Neural Net actually is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute mean binary error ($\\in [0, 100]$) of our best Neural Net model on the test set and store result in `test_err` variable *(1 points)*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK CORRECTNESS\n",
    "assert np.round(test_err, 3) == 2.860"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "BT05-NeuralNet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "103px",
    "width": "252px"
   },
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "509px",
    "left": "0px",
    "right": "1212px",
    "top": "106px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
