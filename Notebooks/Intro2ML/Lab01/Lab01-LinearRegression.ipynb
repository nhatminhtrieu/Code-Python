{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dw29NSYmFpyS"
   },
   "source": [
    "# Lab01: Linear Regression.\n",
    "\n",
    "- Student ID: 21127112\n",
    "- Student name: Triệu Nhật Minh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oHR1Zj5GFpyT"
   },
   "source": [
    "**How to do your homework**\n",
    "\n",
    "\n",
    "You will work directly on this notebook; the word `TODO` indicate the parts you need to do.\n",
    "\n",
    "You can discuss ideas with classmates as well as finding information from the internet, book, etc...; but *this homework must be your*.\n",
    "\n",
    "**How to submit your homework**\n",
    "\n",
    "Before submitting, rerun the notebook (`Kernel` ->` Restart & Run All`).\n",
    "\n",
    "Rename your notebook with `ID.ipynb` (for example, if your ID is 1234567, rename your notebook with `1234567.ipynb`) and submit it on moodle.\n",
    "\n",
    "**Contents:**\n",
    "\n",
    "- Linear Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "REHkv-y8FpyU"
   },
   "source": [
    "### 1. The hypothesis set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e75OMY0KFpyU"
   },
   "source": [
    "- Linear regression is a **linear** model, e.g. a model that assumes a linear relationship between the input variables (x) and the single output variable (y). More specifically, that y can be calculated from a linear combination of the input variables (x).\n",
    "- Generally, a linear model will make predictions by calculating a weighted sum of the input features (independent variables). \n",
    "$$ \\hat{y}=w_0+w_1x_1+w_2x_2+...+w_nx_n $$\n",
    "    - $\\hat{y}$ is the predicted value.\n",
    "    - $n$ is the number of features.\n",
    "    - $x_i$ is the $i^{th}$ feature value.\n",
    "    - $w_j$ is the $j^{th}$ model parameter (including the bias term $w_0$ and the feature weights $w_1,w_2,...w_n)$.\n",
    "$$\\hat{y}=h_{\\mathbf{w}}\\left(\\mathbf{x}\\right)=\\mathbf{w}^{T}\\cdot\\mathbf{x}$$\n",
    "    - $\\mathbf{w}$ is the model **parameter vector** (including the bias term $w_0$ and the feature weights $w_1,w_2,...w_n$).\n",
    "    - $\\mathbf{w}^T$ is a transpose  of $\\mathbf{w}$ (a row vector insteade of column vector).\n",
    "    - $\\mathbf{x}$ is the instance's **feature vector**, *containing* $x_0$ to $x_n$, with $x_0$ *always equal to* 1.\n",
    "    - $\\mathbf{w}^{T}\\cdot\\mathbf{x}$ is the dot product of $\\mathbf{w}^T$ and $\\mathbf{x}$.\n",
    "    - $h_{\\mathbf{w}}$ is the hypothesis function, using the parameters $\\mathbf{w}$.\n",
    "![Bias](Bias.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5l8F4lnjFpyV"
   },
   "source": [
    "### 2. Performance measure and the learning goal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fdJNZ2q6FpyX"
   },
   "source": [
    "- Before we start to train the model, we need to determine how good the model fits the training data. There are a couple of ways to determine the level of quality, but we are going to use the most popular one and that is the **MSE** (Mean Square Error). We need to find the value for $\\mathbf{w}$ that will minimize the MSE:\n",
    "$$\\mathbf{w}=\\arg\\min MSE_{\\mathcal{D}_{train}}$$\n",
    "\n",
    "\n",
    "- MSE on the train set $\\mathcal{D}_{train}$ denoted as $\\left(\\mathbf{X},\\mathbf{y}\\right)$ including m samples $\\left\\{\\left(\\mathbf{x}_1,y_1\\right),\\left(\\mathbf{x}_2,y_2\\right),...\\left(\\mathbf{x}_m,y_m\\right)\\right\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GTOZj7HfFpyY"
   },
   "source": [
    "$$MSE\\left(X,h_{\\mathbf{w}}\\right)=\\dfrac{1}{m}\\sum_{i=1}^{m}\\left(\\mathbf{w}^T\\cdot\\mathbf{x}_i - y_i\\right )^2$$\n",
    "$$MSE\\left(X,h_{\\mathbf{w}}\\right)=\\dfrac{1}{m}\\Vert\\mathbf{X}\\mathbf{w}-\\mathbf{y}\\Vert^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example below is a plot of an MSE function where the true target value is 100, and the predicted values range between -10,000 to 10,000. The MSE loss (Y-axis) reaches its minimum value at prediction (X-axis) = 100. The range is 0 to ∞.\n",
    "![Plot of MSE Loss (Y-axis) vs. Predictions (X-axis)](MSE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORU-9tCxFpyZ"
   },
   "source": [
    "- To find the value of $\\mathbf{w}$ that minimizes the MSE cost function, the most common way (*we have known since high school*) is to solve the derivative (gradient) equation. \n",
    "$$\\mathbf{\\hat{w}}=\\left(\\mathbf{X}^T  \\mathbf{X}\\right)^{\\dagger}  \\mathbf{X}^T  \\mathbf{y}$$\n",
    "  - $\\mathbf{\\hat{w}}$ is the value of $\\mathbf{w}$ that minimizes the cost function\n",
    "  - **Notice that** $\\mathbf{X}^T  \\mathbf{X}$ is not always invertible. $\\left(\\mathbf{X}^T  \\mathbf{X}\\right)^{\\dagger}$ is pseudo-inverse of $\\left(\\mathbf{X}^T \\mathbf{X}\\right)$ - a general case of inverse when the matrix is not invertible or not even square."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Tgy-tRYFpyZ"
   },
   "source": [
    "### 3. Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qauCdk7LFpya"
   },
   "source": [
    "#### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "70Mis-p9Fpyd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import sklearn.datasets as datasets\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nRr06hARFpyk"
   },
   "source": [
    "#### Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g0K3G_SOFpyk"
   },
   "outputs": [],
   "source": [
    "X,y=datasets.make_regression(n_samples=100,n_features=1, noise=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vBFWzeY3Fpyp"
   },
   "source": [
    "#### Load and visualize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BpxLtG3Fpyq"
   },
   "outputs": [],
   "source": [
    "# Visualize data \n",
    "\n",
    "plt.plot(X, y, 'ro')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLDAEVR8Fpyx"
   },
   "source": [
    "**TODO:** \n",
    "\n",
    "- Your observation about data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrb7peM1Fpyz"
   },
   "source": [
    "#### Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DdPXTgoAFpyz"
   },
   "outputs": [],
   "source": [
    "def train_linear_regression(X, y):\n",
    "    '''\n",
    "    Trains Linear Regression on the dataset (X, y).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : numpy array, shape (m, d + 1)\n",
    "        The matrix of input vectors (each row corresponds to an input vector); \n",
    "        the first column of this matrix is all ones (corresponding to x_0).\n",
    "    y : numpy array, shape (m, 1)\n",
    "        The vector of outputs.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : numpy array, shape (d + 1, 1)\n",
    "        The vector of parameters of Linear Regression after training.\n",
    "    '''\n",
    "    # TODO\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDgQ-5EDFpy5"
   },
   "outputs": [],
   "source": [
    "# Construct one_added_X \n",
    "# TODO:\n",
    "# First column of one_added_X is all ones (corresponding to x_0).\n",
    "\n",
    "print ('one_added_X.shape =', one_added_X.shape)\n",
    "print ('y.shape =', y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nVhd2dvCFpzE"
   },
   "source": [
    "#### Train our model and visualize result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3YvmkEEFpzE"
   },
   "outputs": [],
   "source": [
    "w = train_linear_regression(one_added_X, y)\n",
    "\n",
    "# Visualize result\n",
    "predicted_ys = one_added_X.dot(w)\n",
    "\n",
    "plt.plot(X,y,'ro')\n",
    "\n",
    "x_min, x_max = plt.xlim()\n",
    "xs = np.array([x_min, x_max]).reshape(-1, 1)\n",
    "\n",
    "# Construct one_added_xs \n",
    "# TODO:\n",
    "# First column of one_added_xs is all ones (corresponding to x_0).\n",
    "\n",
    "\n",
    "predicted_ys = ones_added_xs.dot(w)\n",
    "plt.plot(xs, predicted_ys)\n",
    "plt.xlim(x_min, x_max)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lTO6ilruFpzH"
   },
   "source": [
    "- **TODO**: Discuss about advantages and disadvantages of `Linear Regression`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BehaTobaFpzI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab01-LinearRegression.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
